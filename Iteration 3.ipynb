{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c5c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/22 23:51:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/22 23:51:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/22 23:51:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('heart-disease-mining-clean') \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6cc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dirt= spark.read.csv('Datasets/heart_2022_with_nans - with dirt.csv', header=True, inferSchema=True)\n",
    "#for column in df_dirt.columns:\n",
    "    #null_count = df_dirt.filter(df_dirt[column].isNull()).count()\n",
    "    #print(f\"Column '{column}': {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1032f3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        SmokerStatus| count|\n",
      "+--------------------+------+\n",
      "|        Never smoked|245955|\n",
      "|       Former smoker|113769|\n",
      "|Current smoker - ...| 36003|\n",
      "|                null| 35462|\n",
      "|Current smoker - ...| 13938|\n",
      "|     Previous smoker|     5|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# use groupby to calculate value counts for 'SmokerStatus'\n",
    "smoker_status_counts = df_dirt.groupBy('SmokerStatus').count().orderBy('count', ascending=False)\n",
    "smoker_status_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f44aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    AgeCategory|count|\n",
      "+---------------+-----+\n",
      "|   Age 65 to 69|47098|\n",
      "|   Age 60 to 64|44511|\n",
      "|   Age 70 to 74|43472|\n",
      "|   Age 55 to 59|36821|\n",
      "|Age 80 or older|36250|\n",
      "|   Age 50 to 54|33644|\n",
      "|   Age 75 to 79|32516|\n",
      "|   Age 40 to 44|29942|\n",
      "|   Age 45 to 49|28531|\n",
      "|   Age 35 to 39|28526|\n",
      "|   Age 18 to 24|26941|\n",
      "|   Age 30 to 34|25806|\n",
      "|   Age 25 to 29|21989|\n",
      "|           null| 9079|\n",
      "|             78|    1|\n",
      "|             67|    1|\n",
      "|             76|    1|\n",
      "|             88|    1|\n",
      "|             34|    1|\n",
      "|             26|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Age_counts = df_dirt.groupBy('AgeCategory').count().orderBy('count', ascending=False)\n",
    "Age_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc37311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|WeightInKilograms|\n",
      "+-------+-----------------+\n",
      "|  count|           403054|\n",
      "|   mean|83.10065152559405|\n",
      "| stddev|27.15749401703654|\n",
      "|    min|            22.68|\n",
      "|    max|          10659.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dirt.describe('WeightInKilograms').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246c85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "columns_to_exclude = [\n",
    "    'State', 'LastCheckupTime', 'ChestScan', 'GeneralHealth',\n",
    "    'PhysicalHealthDays', 'MentalHealthDays', 'RemovedTeeth',\n",
    "    'DeafOrHardOfHearing', 'BlindOrVisionDifficulty',\n",
    "    'DifficultyConcentrating', 'DifficultyWalking',\n",
    "    'DifficultyDressingBathing', 'DifficultyErrands',\n",
    "    'HeightInMeters', 'WeightInKilograms'  \n",
    "]\n",
    "df_03 = df_dirt.drop(*columns_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae9bfa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex',\n",
       " 'PhysicalActivities',\n",
       " 'SleepHours',\n",
       " 'HadHeartAttack',\n",
       " 'HadAngina',\n",
       " 'HadStroke',\n",
       " 'HadAsthma',\n",
       " 'HadSkinCancer',\n",
       " 'HadCOPD',\n",
       " 'HadDepressiveDisorder',\n",
       " 'HadKidneyDisease',\n",
       " 'HadArthritis',\n",
       " 'HadDiabetes',\n",
       " 'SmokerStatus',\n",
       " 'ECigaretteUsage',\n",
       " 'RaceEthnicityCategory',\n",
       " 'AgeCategory',\n",
       " 'BMI',\n",
       " 'AlcoholDrinkers',\n",
       " 'HIVTesting',\n",
       " 'FluVaxLast12',\n",
       " 'PneumoVaxEver',\n",
       " 'TetanusLast10Tdap',\n",
       " 'HighRiskLastYear',\n",
       " 'CovidPos']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_03.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d325bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:==========================================================(2 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 1093\n",
      "SleepHours: 5453\n",
      "HadHeartAttack: 3065\n",
      "HadAngina: 4405\n",
      "HadStroke: 1557\n",
      "HadAsthma: 1773\n",
      "HadSkinCancer: 3143\n",
      "HadCOPD: 2219\n",
      "HadDepressiveDisorder: 2812\n",
      "HadKidneyDisease: 1926\n",
      "HadArthritis: 2633\n",
      "HadDiabetes: 1087\n",
      "SmokerStatus: 35462\n",
      "ECigaretteUsage: 35660\n",
      "RaceEthnicityCategory: 14057\n",
      "AgeCategory: 9079\n",
      "BMI: 48806\n",
      "AlcoholDrinkers: 46574\n",
      "HIVTesting: 66127\n",
      "FluVaxLast12: 47121\n",
      "PneumoVaxEver: 77040\n",
      "TetanusLast10Tdap: 82516\n",
      "HighRiskLastYear: 50623\n",
      "CovidPos: 50764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a list of columns with their respective null counts using isnull\n",
    "null_counts = df_03.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8573a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:00:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 483159 ms exceeds timeout 120000 ms\n",
      "24/05/23 00:00:46 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0.00%\n",
      "PhysicalActivities: 0.25%\n",
      "SleepHours: 1.23%\n",
      "HadHeartAttack: 0.69%\n",
      "HadAngina: 0.99%\n",
      "HadStroke: 0.35%\n",
      "HadAsthma: 0.40%\n",
      "HadSkinCancer: 0.71%\n",
      "HadCOPD: 0.50%\n",
      "HadDepressiveDisorder: 0.63%\n",
      "HadKidneyDisease: 0.43%\n",
      "HadArthritis: 0.59%\n",
      "HadDiabetes: 0.24%\n",
      "SmokerStatus: 7.97%\n",
      "ECigaretteUsage: 8.01%\n",
      "RaceEthnicityCategory: 3.16%\n",
      "AgeCategory: 2.04%\n",
      "BMI: 10.96%\n",
      "AlcoholDrinkers: 10.46%\n",
      "HIVTesting: 14.86%\n",
      "FluVaxLast12: 10.59%\n",
      "PneumoVaxEver: 17.31%\n",
      "TetanusLast10Tdap: 18.54%\n",
      "HighRiskLastYear: 11.37%\n",
      "CovidPos: 11.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the total number of rows in the DataFrame\n",
    "total_count = df_03.count()\n",
    "\n",
    "# Calculate the number of null values and the percentage of nulls for each column\n",
    "null_percentage = df_03.select([\n",
    "    ((F.count(F.when(F.col(c).isNull(), c)) / total_count) * 100).alias(c)\n",
    "    for c in df_03.columns\n",
    "])\n",
    "\n",
    "null_percentage_dict = null_percentage.collect()[0].asDict()\n",
    "\n",
    "for column, percentage in null_percentage_dict.items():\n",
    "    print(f\"{column}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9604e34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 0\n",
      "SleepHours: 2221\n",
      "HadHeartAttack: 0\n",
      "HadAngina: 0\n",
      "HadStroke: 0\n",
      "HadAsthma: 0\n",
      "HadSkinCancer: 0\n",
      "HadCOPD: 0\n",
      "HadDepressiveDisorder: 0\n",
      "HadKidneyDisease: 0\n",
      "HadArthritis: 0\n",
      "HadDiabetes: 0\n",
      "SmokerStatus: 0\n",
      "ECigaretteUsage: 0\n",
      "RaceEthnicityCategory: 0\n",
      "AgeCategory: 0\n",
      "BMI: 15757\n",
      "AlcoholDrinkers: 0\n",
      "HIVTesting: 0\n",
      "FluVaxLast12: 0\n",
      "PneumoVaxEver: 0\n",
      "TetanusLast10Tdap: 0\n",
      "HighRiskLastYear: 0\n",
      "CovidPos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# List of categorical columns to check for null values\n",
    "categorical_columns = [\n",
    "    'Sex', 'PhysicalActivities', 'HadHeartAttack', 'HadAngina', 'HadStroke', 'HadAsthma', \n",
    "    'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder', 'HadKidneyDisease', 'HadArthritis', \n",
    "    'HadDiabetes', 'SmokerStatus', 'ECigaretteUsage', 'RaceEthnicityCategory', 'AgeCategory', \n",
    "    'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver', 'TetanusLast10Tdap', \n",
    "    'HighRiskLastYear', 'CovidPos'\n",
    "]\n",
    "\n",
    "df_03_cleaned_01= df_03.na.drop(subset=categorical_columns)\n",
    "\n",
    "# Count null values in each column after dropping rows\n",
    "null_counts = df_03_cleaned_01.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03_cleaned_01.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "# Print the null counts for each column\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dcb016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 0\n",
      "SleepHours: 0\n",
      "HadHeartAttack: 0\n",
      "HadAngina: 0\n",
      "HadStroke: 0\n",
      "HadAsthma: 0\n",
      "HadSkinCancer: 0\n",
      "HadCOPD: 0\n",
      "HadDepressiveDisorder: 0\n",
      "HadKidneyDisease: 0\n",
      "HadArthritis: 0\n",
      "HadDiabetes: 0\n",
      "SmokerStatus: 0\n",
      "ECigaretteUsage: 0\n",
      "RaceEthnicityCategory: 0\n",
      "AgeCategory: 0\n",
      "BMI: 0\n",
      "AlcoholDrinkers: 0\n",
      "HIVTesting: 0\n",
      "FluVaxLast12: 0\n",
      "PneumoVaxEver: 0\n",
      "TetanusLast10Tdap: 0\n",
      "HighRiskLastYear: 0\n",
      "CovidPos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Calculate the mean of BMI and SleepHours\n",
    "bmi_mean = df_03_cleaned_01.select(mean('BMI')).first()[0]\n",
    "sleep_hours_mean = df_03_cleaned_01.select(mean('SleepHours')).first()[0]\n",
    "\n",
    "# Replace null values with the mean\n",
    "df_03_cleaned_02 = df_03_cleaned_01.fillna({'BMI': bmi_mean, 'SleepHours': sleep_hours_mean})\n",
    "\n",
    "# Verify the changes by counting null values again\n",
    "null_counts = df_03_cleaned_02.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03_cleaned_02.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "# Print the null counts for each column\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5595d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns before filtering: 288444 rows x 25 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns after filtering: 285788 rows x 25 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bmi_lower_bound = df_03_cleaned_02.approxQuantile(\"BMI\", [0.0025], 0.0)[0]\n",
    "bmi_upper_bound = df_03_cleaned_02.approxQuantile(\"BMI\", [0.9975], 0.0)[0]\n",
    "\n",
    "sleep_lower_bound = df_03_cleaned_02.approxQuantile(\"SleepHours\", [0.0025], 0.0)[0]\n",
    "sleep_upper_bound = df_03_cleaned_02.approxQuantile(\"SleepHours\", [0.9975], 0.0)[0]\n",
    "\n",
    "# Filter the dataframe based on the calculated bounds\n",
    "df_03_cleaned_03 = df_03_cleaned_02.filter(\n",
    "    (F.col('BMI') >= bmi_lower_bound) & (F.col('BMI') <= bmi_upper_bound) &\n",
    "    (F.col('SleepHours') >= sleep_lower_bound) & (F.col('SleepHours') <= sleep_upper_bound)\n",
    ")\n",
    "\n",
    "# Show the number of rows and columns after filtering\n",
    "print(f\"Number of rows and columns before filtering: {df_03_cleaned_02.count()} rows x {len(df_03_cleaned_02.columns)} columns\")\n",
    "print(f\"Number of rows and columns after filtering: {df_03_cleaned_03.count()} rows x {len(df_03_cleaned_03.columns)} columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3131ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               BMI|\n",
      "+-------+------------------+\n",
      "|  count|            285788|\n",
      "|   mean|28.596861735345282|\n",
      "| stddev| 6.059249795657394|\n",
      "|    min|             16.24|\n",
      "|    max|             57.39|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_03_cleaned_03.describe('BMI').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed7e4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|    AgeCategory|\n",
      "+---------------+\n",
      "|   Age 45 to 49|\n",
      "|   Age 25 to 29|\n",
      "|   Age 70 to 74|\n",
      "|   Age 55 to 59|\n",
      "|   Age 18 to 24|\n",
      "|   Age 60 to 64|\n",
      "|   Age 50 to 54|\n",
      "|   Age 35 to 39|\n",
      "|   Age 30 to 34|\n",
      "|Age 80 or older|\n",
      "|   Age 40 to 44|\n",
      "|   Age 75 to 79|\n",
      "|   Age 65 to 69|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to reclassify numerical ages to age categories\n",
    "def reclassify_age(age):\n",
    "    if age <= 25:\n",
    "        return 'Age 18 to 24'\n",
    "    elif 25 < age <= 30:\n",
    "        return 'Age 25 to 29'\n",
    "    elif 30 < age <= 35:\n",
    "        return 'Age 30 to 34'\n",
    "    elif 35 < age <= 40:\n",
    "        return 'Age 35 to 39'\n",
    "    elif 40 < age <= 45:\n",
    "        return 'Age 40 to 44'\n",
    "    elif 45 < age <= 50:\n",
    "        return 'Age 45 to 49'\n",
    "    elif 50 < age <= 55:\n",
    "        return 'Age 50 to 54'\n",
    "    elif 55 < age <= 60:\n",
    "        return 'Age 55 to 59'\n",
    "    elif 60 < age <= 65:\n",
    "        return 'Age 60 to 64'\n",
    "    elif 65 < age <= 70:\n",
    "        return 'Age 65 to 69'\n",
    "    elif 70 < age <= 75:\n",
    "        return 'Age 70 to 74'\n",
    "    elif 75 < age <= 80:\n",
    "        return 'Age 75 to 79'\n",
    "    else:\n",
    "        return 'Age 80 or older'\n",
    "\n",
    "# UDF to apply the reclassification function\n",
    "reclassify_age_udf = F.udf(lambda x: reclassify_age(int(x)) if x.isdigit() else x, StringType())\n",
    "df_03_cleaned_04 = df_03_cleaned_03.withColumn('AgeCategory', reclassify_age_udf(F.col('AgeCategory')))\n",
    "\n",
    "# Show the reclassified age category\n",
    "df_03_cleaned_04.select('AgeCategory').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb0c7c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        SmokerStatus|\n",
      "+--------------------+\n",
      "|        Never smoked|\n",
      "|Current smoker - ...|\n",
      "|       Former smoker|\n",
      "|Current smoker - ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Function to reclassify smoker status\n",
    "def reclassify_smoker(status):\n",
    "    if status == 'Previous smoker':\n",
    "        return 'Former smoker'\n",
    "    else:\n",
    "        return status\n",
    "\n",
    "# UDF to apply the reclassification function\n",
    "reclassify_smoker_udf = F.udf(lambda x: reclassify_smoker(x), StringType())\n",
    "\n",
    "# Apply the UDF to reclassify smoker status\n",
    "df_03_cleaned_05 = df_03_cleaned_04.withColumn('SmokerStatus', reclassify_smoker_udf(F.col('SmokerStatus')))\n",
    "\n",
    "# Show the reclassified DataFrame\n",
    "df_03_cleaned_05.select('SmokerStatus').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cb61ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------------+\n",
      "|HadHeartAttack|HadAngina|HadHeartDisease|\n",
      "+--------------+---------+---------------+\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|           Yes|       No|            Yes|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|           Yes|      Yes|            Yes|\n",
      "|            No|       No|             No|\n",
      "|            No|      Yes|            Yes|\n",
      "|            No|       No|             No|\n",
      "|           Yes|      Yes|            Yes|\n",
      "+--------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Function to determine if a respondent had heart disease\n",
    "def had_heart_disease(had_heart_attack, had_angina):\n",
    "    return 'Yes' if had_heart_attack == 'Yes' or had_angina == 'Yes' else 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_heart_disease_udf = F.udf(lambda x, y: had_heart_disease(x, y), StringType())\n",
    "\n",
    "# Apply the UDF to create the HadHeartDisease column\n",
    "df_03_cleaned_06 = df_03_cleaned_05.withColumn('HadHeartDisease', had_heart_disease_udf(F.col('HadHeartAttack'), F.col('HadAngina')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_06.select('HadHeartAttack', 'HadAngina', 'HadHeartDisease').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ab572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|     ECigaretteUsage|EcigaFlag|\n",
      "+--------------------+---------+\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition two: EcigaFlag column\n",
    "\n",
    "# Function to determine the e-cigarette usage flag\n",
    "def ecig_new(response):\n",
    "    if response in [\"Use them every day\", \"Use them some days\"]:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "ecig_new_udf = F.udf(lambda x: ecig_new(x), StringType())\n",
    "\n",
    "# Apply the UDF to create the EcigaFlag column\n",
    "df_03_cleaned_07 = df_03_cleaned_06.withColumn('EcigaFlag', ecig_new_udf(F.col('ECigaretteUsage')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_07.select('ECigaretteUsage', 'EcigaFlag').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be6dc244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        SmokerStatus|Smoked|\n",
      "+--------------------+------+\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|Current smoker - ...|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|Current smoker - ...|   Yes|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition three: Smoked column\n",
    "# Function to determine if the person has smoked\n",
    "def smoker_new(status):\n",
    "    if status == \"Never smoked\":\n",
    "        return \"No\"\n",
    "    else:\n",
    "        return \"Yes\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "smoker_new_udf = F.udf(lambda x: smoker_new(x), StringType())\n",
    "\n",
    "# Apply the UDF to create the Smoked column\n",
    "df_03_cleaned_08 = df_03_cleaned_07.withColumn('Smoked', smoker_new_udf(F.col('SmokerStatus')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_08.select('SmokerStatus', 'Smoked').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3530475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|HadDiabetes|Diabetic|\n",
      "+-----------+--------+\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition four: Diabetic column\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "# Function to determine if the person is diabetic\n",
    "def diabetes_new(status):\n",
    "    if status == 'Yes' or status == 'Yes, but only during pregnancy (female)':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "diabetes_new_udf = udf(diabetes_new, StringType())\n",
    "\n",
    "# Apply the UDF to create the Diabetic column\n",
    "df_03_cleaned_09 = df_03_cleaned_08.withColumn('Diabetic', diabetes_new_udf(F.col('HadDiabetes')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_09.select('HadDiabetes', 'Diabetic').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb94c244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Diabetic|\n",
      "+--------+\n",
      "|      No|\n",
      "|     Yes|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_03_cleaned_09.select('Diabetic').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3e443a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|CovidPos|HadCovid|\n",
      "+--------+--------+\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|     Yes|     Yes|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|     Yes|     Yes|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition five: had_covid\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to determine if the person had Covid\n",
    "def had_covid(status):\n",
    "    if status == 'Yes' or status == 'Tested positive using home test without a health professional':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_covid_udf = udf(had_covid, StringType())\n",
    "\n",
    "# Apply the UDF to create the HadCovid column\n",
    "df_03_cleaned_10 = df_03_cleaned_09.withColumn('HadCovid', had_covid_udf(F.col('CovidPos')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_10.select('CovidPos', 'HadCovid').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7123c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|   TetanusLast10Tdap|HadTetanus|\n",
      "+--------------------+----------+\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|  Yes, received Tdap|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition six\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to determine if the person had tetanus\n",
    "def had_tetanus(status):\n",
    "    if status == 'Yes, received tetanus shot but not sure what type' or status == 'Yes, received Tdap':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_tetanus_udf = udf(had_tetanus, StringType())\n",
    "\n",
    "# Apply the UDF to create the HadTetanus column\n",
    "df_03_cleaned_11 = df_03_cleaned_10.withColumn('HadTetanus', had_tetanus_udf(F.col('TetanusLast10Tdap')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_11.select('TetanusLast10Tdap', 'HadTetanus').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b19a66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|RaceEthnicityCategory|RaceEth|\n",
      "+---------------------+-------+\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "+---------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition seven\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to reclassify race and ethnicity\n",
    "def race_ethnicity(category):\n",
    "    if category == 'White only, Non-Hispanic':\n",
    "        return 'White'\n",
    "    elif category == 'Black only, Non-Hispanic':\n",
    "        return 'Black'\n",
    "    elif category == 'Other race only, Non-Hispanic':\n",
    "        return 'Other'\n",
    "    elif category == 'Hispanic':\n",
    "        return 'Hispanic'\n",
    "    elif category == 'Multiracial, Non-Hispanic':\n",
    "        return 'Multiracial'\n",
    "    else:\n",
    "        return category\n",
    "\n",
    "# Register the function as a UDF\n",
    "race_ethnicity_udf = udf(race_ethnicity, StringType())\n",
    "\n",
    "# Apply the UDF to create the RaceEth column\n",
    "df_03_cleaned_12 = df_03_cleaned_11.withColumn('RaceEth', race_ethnicity_udf(F.col('RaceEthnicityCategory')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_12.select('RaceEthnicityCategory', 'RaceEth').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c1bd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- PhysicalActivities: string (nullable = true)\n",
      " |-- SleepHours: integer (nullable = true)\n",
      " |-- HadStroke: string (nullable = true)\n",
      " |-- HadAsthma: string (nullable = true)\n",
      " |-- HadSkinCancer: string (nullable = true)\n",
      " |-- HadCOPD: string (nullable = true)\n",
      " |-- HadDepressiveDisorder: string (nullable = true)\n",
      " |-- HadKidneyDisease: string (nullable = true)\n",
      " |-- HadArthritis: string (nullable = true)\n",
      " |-- AgeCategory: string (nullable = true)\n",
      " |-- BMI: double (nullable = false)\n",
      " |-- AlcoholDrinkers: string (nullable = true)\n",
      " |-- HIVTesting: string (nullable = true)\n",
      " |-- FluVaxLast12: string (nullable = true)\n",
      " |-- PneumoVaxEver: string (nullable = true)\n",
      " |-- HighRiskLastYear: string (nullable = true)\n",
      " |-- HadHeartDisease: string (nullable = true)\n",
      " |-- EcigaFlag: string (nullable = true)\n",
      " |-- Smoked: string (nullable = true)\n",
      " |-- Diabetic: string (nullable = true)\n",
      " |-- HadCovid: string (nullable = true)\n",
      " |-- HadTetanus: string (nullable = true)\n",
      " |-- RaceEth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.5 Data formatting\n",
    "cleaned_df = df_03_cleaned_12\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"ECigaretteUsage\", \"SmokerStatus\", \n",
    "                   \"HadHeartAttack\", \"HadAngina\", \"HadDiabetes\", \"CovidPos\", \"TetanusLast10Tdap\", \"RaceEthnicityCategory\"]\n",
    "\n",
    "# Drop the columns\n",
    "formatted_df_01 = cleaned_df.drop(*columns_to_drop)\n",
    "\n",
    "# Show the schema to verify columns are dropped\n",
    "formatted_df_01.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a4791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Sex to Index:\n",
      "  0 -> Female\n",
      "  1 -> Male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of PhysicalActivities to Index:\n",
      "  0 -> Yes\n",
      "  1 -> No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadStroke to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:35:55 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from ip-172-31-0-34.ec2.internal:38731 in 120 seconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadAsthma to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadSkinCancer to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadCOPD to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadDepressiveDisorder to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadKidneyDisease to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadArthritis to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of AlcoholDrinkers to Index:\n",
      "  0 -> Yes\n",
      "  1 -> No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HIVTesting to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of FluVaxLast12 to Index:\n",
      "  0 -> Yes\n",
      "  1 -> No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of PneumoVaxEver to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HighRiskLastYear to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadHeartDisease to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of EcigaFlag to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Smoked to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Diabetic to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadCovid to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadTetanus to Index:\n",
      "  0 -> Yes\n",
      "  1 -> No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of RaceEth to Index:\n",
      "  0 -> White\n",
      "  1 -> Hispanic\n",
      "  2 -> Black\n",
      "  3 -> Other\n",
      "  4 -> Multiracial\n",
      "Mapping of AgeCategory to Index:\n",
      "  0 -> Age 18 to 24\n",
      "  1 -> Age 25 to 29\n",
      "  2 -> Age 30 to 34\n",
      "  3 -> Age 35 to 39\n",
      "  4 -> Age 40 to 44\n",
      "  5 -> Age 45 to 49\n",
      "  6 -> Age 50 to 54\n",
      "  7 -> Age 55 to 59\n",
      "  8 -> Age 60 to 64\n",
      "  9 -> Age 65 to 69\n",
      "  10 -> Age 70 to 74\n",
      "  11 -> Age 75 to 79\n",
      "  12 -> Age 80 or older\n",
      "root\n",
      " |-- SleepHours: integer (nullable = true)\n",
      " |-- BMI: double (nullable = false)\n",
      " |-- Sex: double (nullable = false)\n",
      " |-- PhysicalActivities: double (nullable = false)\n",
      " |-- HadStroke: double (nullable = false)\n",
      " |-- HadAsthma: double (nullable = false)\n",
      " |-- HadSkinCancer: double (nullable = false)\n",
      " |-- HadCOPD: double (nullable = false)\n",
      " |-- HadDepressiveDisorder: double (nullable = false)\n",
      " |-- HadKidneyDisease: double (nullable = false)\n",
      " |-- HadArthritis: double (nullable = false)\n",
      " |-- AlcoholDrinkers: double (nullable = false)\n",
      " |-- HIVTesting: double (nullable = false)\n",
      " |-- FluVaxLast12: double (nullable = false)\n",
      " |-- PneumoVaxEver: double (nullable = false)\n",
      " |-- HighRiskLastYear: double (nullable = false)\n",
      " |-- HadHeartDisease: double (nullable = false)\n",
      " |-- EcigaFlag: double (nullable = false)\n",
      " |-- Smoked: double (nullable = false)\n",
      " |-- Diabetic: double (nullable = false)\n",
      " |-- HadCovid: double (nullable = false)\n",
      " |-- HadTetanus: double (nullable = false)\n",
      " |-- RaceEth: double (nullable = false)\n",
      " |-- AgeCategory: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 124:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---+------------------+---------+---------+-------------+-------+---------------------+----------------+------------+---------------+----------+------------+-------------+----------------+---------------+---------+------+--------+--------+----------+-------+-----------+\n",
      "|SleepHours|               BMI|Sex|PhysicalActivities|HadStroke|HadAsthma|HadSkinCancer|HadCOPD|HadDepressiveDisorder|HadKidneyDisease|HadArthritis|AlcoholDrinkers|HIVTesting|FluVaxLast12|PneumoVaxEver|HighRiskLastYear|HadHeartDisease|EcigaFlag|Smoked|Diabetic|HadCovid|HadTetanus|RaceEth|AgeCategory|\n",
      "+----------+------------------+---+------------------+---------+---------+-------------+-------+---------------------+----------------+------------+---------------+----------+------------+-------------+----------------+---------------+---------+------+--------+--------+----------+-------+-----------+\n",
      "|         8|28.652459119794525|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         0.0|          0.0|             0.0|            0.0|      0.0|   0.0|     1.0|     0.0|       0.0|    0.0|         12|\n",
      "|         6|             26.57|0.0|               1.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         1.0|          0.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       1.0|    0.0|         12|\n",
      "|         9|             21.77|0.0|               0.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            0.0|       0.0|         1.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       1.0|    0.0|          4|\n",
      "|         7|             26.08|1.0|               1.0|      1.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         1.0|          1.0|             0.0|            1.0|      0.0|   0.0|     1.0|     0.0|       1.0|    0.0|         12|\n",
      "|         7|             22.96|0.0|               0.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            0.0|       0.0|         1.0|          0.0|             0.0|            0.0|      0.0|   1.0|     0.0|     0.0|       1.0|    2.0|         12|\n",
      "|         8|             27.81|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|    0.0|         12|\n",
      "|         6|28.652459119794525|0.0|               0.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       1.0|         1.0|          0.0|             0.0|            0.0|      0.0|   1.0|     0.0|     0.0|       0.0|    0.0|         11|\n",
      "|         8|             29.23|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|    0.0|         12|\n",
      "|         6|             23.21|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|    0.0|         12|\n",
      "|         6|             28.59|1.0|               1.0|      0.0|      1.0|          0.0|    0.0|                  0.0|             0.0|         1.0|            0.0|       1.0|         0.0|          0.0|             0.0|            0.0|      0.0|   1.0|     0.0|     0.0|       1.0|    2.0|          7|\n",
      "|         8|             25.34|0.0|               0.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         1.0|            0.0|       0.0|         1.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|    0.0|         12|\n",
      "|         6|             32.28|0.0|               0.0|      0.0|      1.0|          1.0|    1.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   1.0|     1.0|     1.0|       1.0|    0.0|         12|\n",
      "|         8|             44.59|0.0|               1.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             1.0|         1.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       1.0|    0.0|         12|\n",
      "|         8|             24.34|1.0|               0.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         0.0|          0.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|    0.0|          9|\n",
      "|         6|             21.63|0.0|               0.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         1.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|    0.0|         11|\n",
      "|         8|             37.45|0.0|               0.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         1.0|            0.0|       0.0|         0.0|          1.0|             0.0|            1.0|      0.0|   1.0|     1.0|     1.0|       0.0|    2.0|          9|\n",
      "|         8|             31.09|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  1.0|             0.0|         1.0|            1.0|       0.0|         0.0|          0.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       1.0|    0.0|         11|\n",
      "|         8|             22.14|0.0|               1.0|      0.0|      0.0|          0.0|    1.0|                  0.0|             1.0|         0.0|            1.0|       0.0|         0.0|          0.0|             0.0|            1.0|      0.0|   1.0|     0.0|     0.0|       1.0|    0.0|          9|\n",
      "|         8|             29.12|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   1.0|     1.0|     0.0|       1.0|    0.0|         10|\n",
      "|         4|             24.39|1.0|               0.0|      1.0|      0.0|          1.0|    0.0|                  1.0|             1.0|         1.0|            0.0|       1.0|         0.0|          1.0|             0.0|            1.0|      0.0|   0.0|     0.0|     0.0|       0.0|    0.0|         11|\n",
      "+----------+------------------+---+------------------+---------+---------+-------------+-------+---------------------+----------------+------------+---------------+----------+------------+-------------+----------------+---------------+---------+------+--------+--------+----------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Define all categorical columns, excluding AgeCategory for manual indexing\n",
    "categorical_columns = [\n",
    "    'Sex', 'PhysicalActivities', 'HadStroke', 'HadAsthma', 'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder',\n",
    "    'HadKidneyDisease', 'HadArthritis', 'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver',\n",
    "    'HighRiskLastYear', 'HadHeartDisease', 'EcigaFlag', 'Smoked', 'Diabetic', 'HadCovid', 'HadTetanus', 'RaceEth'\n",
    "]\n",
    "\n",
    "# Apply StringIndexer for all categorical columns except AgeCategory\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_Index\") for column in categorical_columns]\n",
    "\n",
    "# Fit the StringIndexer models and transform the data\n",
    "for indexer in indexers:\n",
    "    model = indexer.fit(formatted_df_01)\n",
    "    formatted_df_01 = model.transform(formatted_df_01)\n",
    "    \n",
    "    # Print the labels and their corresponding indices\n",
    "    labels = model.labels\n",
    "    print(f\"Mapping of {indexer.getInputCol()} to Index:\")\n",
    "    for index, label in enumerate(labels):\n",
    "        print(f\"  {index} -> {label}\")\n",
    "\n",
    "# Manual encoding for AgeCategory\n",
    "age_category_mapping = {\n",
    "    'Age 18 to 24': 0,\n",
    "    'Age 25 to 29': 1,\n",
    "    'Age 30 to 34': 2,\n",
    "    'Age 35 to 39': 3,\n",
    "    'Age 40 to 44': 4,\n",
    "    'Age 45 to 49': 5,\n",
    "    'Age 50 to 54': 6,\n",
    "    'Age 55 to 59': 7,\n",
    "    'Age 60 to 64': 8,\n",
    "    'Age 65 to 69': 9,\n",
    "    'Age 70 to 74': 10,\n",
    "    'Age 75 to 79': 11,\n",
    "    'Age 80 or older': 12\n",
    "}\n",
    "\n",
    "# Create a new column AgeCategory_Index based on the manual mapping\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def age_category_index(age_category):\n",
    "    return age_category_mapping[age_category]\n",
    "\n",
    "# Register the UDF\n",
    "age_category_index_udf = udf(age_category_index, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the new column\n",
    "formatted_df_01 = formatted_df_01.withColumn(\"AgeCategory_Index\", age_category_index_udf(formatted_df_01[\"AgeCategory\"]))\n",
    "\n",
    "# Print the manual mapping\n",
    "print(\"Mapping of AgeCategory to Index:\")\n",
    "for label, index in age_category_mapping.items():\n",
    "    print(f\"  {index} -> {label}\")\n",
    "\n",
    "# Drop the original categorical columns and rename the indexed columns\n",
    "for column in categorical_columns:\n",
    "    formatted_df_01 = formatted_df_01.drop(column).withColumnRenamed(column + \"_Index\", column)\n",
    "\n",
    "# Drop the original AgeCategory column\n",
    "formatted_df_01 = formatted_df_01.drop('AgeCategory').withColumnRenamed('AgeCategory_Index', 'AgeCategory')\n",
    "\n",
    "# Show the schema to confirm changes\n",
    "formatted_df_01.printSchema()\n",
    "\n",
    "# Verify the first few rows to confirm changes\n",
    "formatted_df_01.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad441c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|HadHeartDisease| count|\n",
      "+---------------+------+\n",
      "|            0.0|260329|\n",
      "|            1.0| 25459|\n",
      "+---------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "formatted_df_01.groupBy('HadHeartDisease').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "277e1523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SleepHours: integer (nullable = true)\n",
      " |-- BMI: double (nullable = false)\n",
      " |-- Sex: double (nullable = false)\n",
      " |-- PhysicalActivities: double (nullable = false)\n",
      " |-- HadStroke: double (nullable = false)\n",
      " |-- HadAsthma: double (nullable = false)\n",
      " |-- HadSkinCancer: double (nullable = false)\n",
      " |-- HadCOPD: double (nullable = false)\n",
      " |-- HadDepressiveDisorder: double (nullable = false)\n",
      " |-- HadKidneyDisease: double (nullable = false)\n",
      " |-- HadArthritis: double (nullable = false)\n",
      " |-- AlcoholDrinkers: double (nullable = false)\n",
      " |-- HIVTesting: double (nullable = false)\n",
      " |-- FluVaxLast12: double (nullable = false)\n",
      " |-- PneumoVaxEver: double (nullable = false)\n",
      " |-- HadHeartDisease: double (nullable = false)\n",
      " |-- Smoked: double (nullable = false)\n",
      " |-- Diabetic: double (nullable = false)\n",
      " |-- RaceEth: double (nullable = false)\n",
      " |-- AgeCategory: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formatted_df = formatted_df_01\n",
    "features_to_drop = ['HadTetanus', 'HadCovid', 'EcigaFlag', 'HighRiskLastYear', 'features']\n",
    "\n",
    "# Drop the features\n",
    "reduced_df = formatted_df.drop(*features_to_drop)\n",
    "\n",
    "# Verify the remaining columns\n",
    "reduced_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4880015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAGDCAYAAABJITbwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhTklEQVR4nO3de5jdVX3v8feHRLxxVVKKgITW1JraihqVVm2peLhVG229oK1ESkUfb7WiFW8HKtJqT6stVVGUHMALiPd4jCIiFW0FCYggKDVFkCBCIBHwLvg9f+w1sBlmJpPLnklW3q/n2c/s/V2/31przwzh86z1+81OVSFJkqQ+bDPbE5AkSdKmY7iTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTtrKJXl3kjduor4enORHSea01/+R5K83Rd+tv88mWbKp+luPcd+c5KYkP5ih8Y5N8oGZGGtTSvLEJFfO9jykrZ3hTupYkquT/DTJbUl+mOS/krwoyZ3/7VfVi6rquGn29eSpjqmq71XVdlV1xyaY+z0CTlUdXFWnbmzf6zmPBwNHAQur6tcnaN8vyaoJ6psk2CaZn6SSzB1XPyXJmze2/0nGvMd7aj+PX7bfpduS/HeSdyTZbeyYqvpyVT10FHOSNH2GO6l/T62q7YG9gLcArwFO3tSDjA8fHXkwcHNV3TjbE5kJ6/g5frj9Lj0AeDrw68BFwwFP0uwz3Elbiaq6paqWAc8GliR5ONx9BSjJLkn+X1vlW5Pky0m2SfJ+BiHn023b9e+GVpSOSPI94IuTrDL9ZpKvJbk1yaeSPKCNNdHq0NVJnpzkIOB1wLPbeN9o7XeuhrV5vSHJNUluTHJakh1b29g8liT5XttSff1k35skO7bzV7f+3tD6fzJwNvCgNo9TNuR7n2Tn9n1dnWRte77HUPveSb7UVsTOBnbZgDH2bSuzP0zyjST7DbUdnuRbrf+rkrxwqG2/JKuSvKZtO58OfHboPf8oyYOGx6qqX1bV5Qx+l1YzWNm8x8+09XldG/fKJPu3+jZJjk7yP0luTnLm2O9Fa/9Ikh8kuSXJeUl+Z6jtkCRXtD6vS/KqobanJLkkd61S/976fh+lHhjupK1MVX0NWAU8cYLmo1rbPGBXBgGrqup5wPcYrAJuV1X/NHTOHwEPAw6cZMjDgL8CdgNuB06Yxhw/B/wDg5Wi7arqERMc9vz2+GPgN4DtgHeMO+YJwEOB/YH/neRhkwz578COrZ8/anM+vKq+ABwMfL/N4/nrmvsktgH+L4PV0wcDPx031w8BFzEIdccB63VdYZLdgc8Ab2awqvYq4GNJ5rVDbgSeAuwAHA68Pcmjhrr49XbeXgze+/B73q6qvj/RuG37/VNM8LuU5KHAS4HHtNW+A4GrW/PLgKcx+F4/CFgLvHPo9M8CC4BfAy4GPjjUdjLwwtbnw4EvtvEeCSwFXgg8EHgPsCzJvSeau9Qzw520dfo+g/+Zj/dLBiFsr7Y68+Va9wdQH1tVP66qn07S/v6q+mZV/Rh4I/CstBsuNtJfAG+rqquq6kfAa4FDx60a/n1V/bSqvgF8A7hHSGxzORR4bVXdVlVXA/8CPG895vKgtlp054NBsASgqm6uqo9V1U+q6jbgeAbBZuyavscAb6yqn1fVecCnJxjjpnH9P3eo7S+B5VW1vKp+VVVnAyuAQ9r4n6mq/6mBLwGf5+6B7FfAMW38yX6Ok5nsd+kO4N7AwiT3qqqrq+p/WtuLgNdX1aqq+jlwLPCMsZ9dVS1tP4uxtkeMrcoy+B1dmGSHqlpbVRe3+pHAe6rqgqq6o12b+XNg3/V8P9IWz3AnbZ12B9ZMUP8/wErg82377uhp9HXterRfA9yLDdh2nMCDWn/Dfc9lsOI4Zvju1p8wWN0bb5c2p/F97b4ec/l+Ve00/AC+MtaY5H5J3tO2fG8FzgN2asHyQcDaFn6Hx7/HPMf1/6Ghtr2AZ04QLndr4x+c5PwMttp/yCD0Df8MVlfVz9bj/Q6b8HepqlYCr2AQzm5McsbQ9u5ewCeG5votBmFw1yRzkrylbdneyl2rfWPz/fM2/2vaVvbvD/V51LjvwZ4Mvr/SVsVwJ21lkjyGwf+QvzK+ra2WHFVVvwH8KfDKseukgMlW8Na1srfn0PMHM1h5uQn4MXC/oXnNYbAdPN1+v8/gf+jDfd8O3LCO88a7qc1pfF/XrWc/UzmKwfbw46pqB+APWz3A9cDOSe4/bvz1cS2DFdLhgHn/qnpL25b8GPDPwK4tGC5vY48Z/71e1/d+MPnBXddPBb48UXtVfaiqnsDge1vAW4fme/C4+d6nqq5jsCK5GHgyg63y+WPDtT4vrKrFDLZsPwmcOdTn8eP6vF9VnT6d9yL1xHAnbSWS7JDkKcAZwAeq6rIJjnlKkockCXALg9WUX7XmGxhck7a+/jLJwiT3A94EfLRdq/XfwH2S/EmSewFvYLCNN+YGYH6G/mzLOKcDf9tuRtiOu67Ru319JtfmciZwfJLtk+wFvBLYlH9nbnsG19n9sN04cMzQ+Ncw2EL9+yTbJnkCg8C0Pj4APDXJgW3l6z7t5oY9gG0ZfF9XA7cnORg4YB393QA8cGgr9G6SzG3XL57O4Hq9t01wzEOTPKmFy58xeP9jv0vvZvD93qsdOy/J4ta2PYPt1JsZhP9/GOpz2yR/kWTHqvolcOtQn+8FXpTkcRm4f/vd2n4d71XqjuFO6t+nk9zGYGXj9Qz+R3z4JMcuAL4A/Aj4KvCuqjq3tf0j8Ia25fWqSc6fyPuBUxhskd4HeDkM7t4FXgy8j8Eq2Y8Z3Mwx5iPt681JLuaelra+zwO+yyBAvGw95jXsZW38qxisaH6o9b+p/CtwXwarhOcDnxvX/lzgcQy2N48BTlufzqvqWgarXa9jEOKuBV4NbNOu8Xs5gwC7to21bB39fZtBcLuq/bzHtjafneRHDIL/MgYB7NGT3HBxbwZ/eucmBj/7X2NwXSTAv7XzP99+N89v75/23q9h8DtxRWsb9jzg6rZl+yIG115SVSuAFzC4UWUtg8sLnj/V+5R6lXVfKy1JkqQthSt3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR2Zu+5Dtg677LJLzZ8/f7anIUmStE4XXXTRTVU1b6I2w10zf/58VqxYMdvTkCRJWqckE31MIeC2rCRJUlcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkfmzvYEtlZPfOFxsz0Faav05fe8cbanIEkj5cqdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktSRkYW7JHsmOTfJFUkuT/I3rX5skuuSXNIehwyd89okK5NcmeTAofpBrbYyydFD9b2TXNDqH06ybavfu71e2drnj+p9SpIkbU5GuXJ3O3BUVS0E9gVekmRha3t7Ve3THssBWtuhwO8ABwHvSjInyRzgncDBwELgOUP9vLX19RBgLXBEqx8BrG31t7fjJEmSujeycFdV11fVxe35bcC3gN2nOGUxcEZV/byqvgusBB7bHiur6qqq+gVwBrA4SYAnAR9t558KPG2or1Pb848C+7fjJUmSujYj19y1bdFHAhe00kuTXJpkaZKdW2134Nqh01a12mT1BwI/rKrbx9Xv1ldrv6UdP35eRyZZkWTF6tWrN+5NSpIkbQZGHu6SbAd8DHhFVd0KnAj8JrAPcD3wL6Oew2Sq6qSqWlRVi+bNmzdb05AkSdpkRhruktyLQbD7YFV9HKCqbqiqO6rqV8B7GWy7AlwH7Dl0+h6tNln9ZmCnJHPH1e/WV2vfsR0vSZLUtVHeLRvgZOBbVfW2ofpuQ4c9Hfhme74MOLTd6bo3sAD4GnAhsKDdGbstg5sullVVAecCz2jnLwE+NdTXkvb8GcAX2/GSJEldm7vuQzbY44HnAZcluaTVXsfgbtd9gAKuBl4IUFWXJzkTuILBnbYvqao7AJK8FDgLmAMsrarLW3+vAc5I8mbg6wzCJO3r+5OsBNYwCISSJEndG1m4q6qvABPdobp8inOOB46foL58ovOq6iru2tYdrv8MeOb6zFeSJKkHfkKFJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdWRk4S7JnknOTXJFksuT/E2rPyDJ2Um+077u3OpJckKSlUkuTfKoob6WtOO/k2TJUP3RSS5r55yQJFONIUmS1LtRrtzdDhxVVQuBfYGXJFkIHA2cU1ULgHPaa4CDgQXtcSRwIgyCGnAM8DjgscAxQ2HtROAFQ+cd1OqTjSFJktS1kYW7qrq+qi5uz28DvgXsDiwGTm2HnQo8rT1fDJxWA+cDOyXZDTgQOLuq1lTVWuBs4KDWtkNVnV9VBZw2rq+JxpAkSerajFxzl2Q+8EjgAmDXqrq+Nf0A2LU93x24dui0Va02VX3VBHWmGGP8vI5MsiLJitWrV2/AO5MkSdq8jDzcJdkO+Bjwiqq6dbitrbjVKMefaoyqOqmqFlXVonnz5o1yGpIkSTNipOEuyb0YBLsPVtXHW/mGtqVK+3pjq18H7Dl0+h6tNlV9jwnqU40hSZLUtVHeLRvgZOBbVfW2oaZlwNgdr0uATw3VD2t3ze4L3NK2Vs8CDkiyc7uR4gDgrNZ2a5J921iHjetrojEkSZK6NneEfT8eeB5wWZJLWu11wFuAM5McAVwDPKu1LQcOAVYCPwEOB6iqNUmOAy5sx72pqta05y8GTgHuC3y2PZhiDEmSpK6NLNxV1VeATNK8/wTHF/CSSfpaCiydoL4CePgE9ZsnGkOSJKl3fkKFJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHRlZuEuyNMmNSb45VDs2yXVJLmmPQ4baXptkZZIrkxw4VD+o1VYmOXqovneSC1r9w0m2bfV7t9crW/v8Ub1HSZKkzc0oV+5OAQ6aoP72qtqnPZYDJFkIHAr8TjvnXUnmJJkDvBM4GFgIPKcdC/DW1tdDgLXAEa1+BLC21d/ejpMkSdoqjCzcVdV5wJppHr4YOKOqfl5V3wVWAo9tj5VVdVVV/QI4A1icJMCTgI+2808FnjbU16nt+UeB/dvxkiRJ3ZuNa+5emuTStm27c6vtDlw7dMyqVpus/kDgh1V1+7j63fpq7be04yVJkro30+HuROA3gX2A64F/meHx7ybJkUlWJFmxevXq2ZyKJEnSJjGj4a6qbqiqO6rqV8B7GWy7AlwH7Dl06B6tNln9ZmCnJHPH1e/WV2vfsR0/0XxOqqpFVbVo3rx5G/v2JEmSZt2Mhrskuw29fDowdiftMuDQdqfr3sAC4GvAhcCCdmfstgxuulhWVQWcCzyjnb8E+NRQX0va82cAX2zHS5IkdW/uug+BJI+vqv9cV21c++nAfsAuSVYBxwD7JdkHKOBq4IUAVXV5kjOBK4DbgZdU1R2tn5cCZwFzgKVVdXkb4jXAGUneDHwdOLnVTwben2Qlgxs6Dp3Oe5QkSerBtMId8O/Ao6ZRu1NVPWeC8skT1MaOPx44foL6cmD5BPWruGtbd7j+M+CZk40jSZLUsynDXZLfB/4AmJfklUNNOzBYSZMkSdJmZF0rd9sC27Xjth+q38pd17tJkiRpMzFluKuqLwFfSnJKVV0zQ3OSJEnSBpruNXf3TnISMH/4nKp60igmJUmSpA0z3XD3EeDdwPuAO0Y3HUmSJG2M6Ya726vqxJHORJIkSRttun/E+NNJXpxktyQPGHuMdGaSJElab9NduRv7xIdXD9UK+I1NOx1JkiRtjGmFu6rae9QTkSRJ0sab7sePHTZRvapO27TTkSRJ0saY7rbsY4ae3wfYH7gYMNxJkiRtRqa7Lfuy4ddJdgLOGMWEJEmStOGme7fseD8GvA5PkiRpMzPda+4+zeDuWIA5wMOAM0c1KUmSJG2Y6V5z989Dz28HrqmqVSOYjyRJkjbCtLZlq+pLwLeB7YGdgV+MclKSJEnaMNMKd0meBXwNeCbwLOCCJM8Y5cQkSZK0/qa7Lft64DFVdSNAknnAF4CPjmpikiRJWn/TvVt2m7Fg19y8HudKkiRphkx35e5zSc4CTm+vnw0sH82UJEmStKGmDHdJHgLsWlWvTvJnwBNa01eBD456cpIkSVo/61q5+1fgtQBV9XHg4wBJfre1PXWEc5MkSdJ6Wtd1c7tW1WXji602fyQzkiRJ0gZbV7jbaYq2+27CeUiSJGkTWFe4W5HkBeOLSf4auGg0U5IkSdKGWtc1d68APpHkL7grzC0CtgWePsJ5SZIkaQNMGe6q6gbgD5L8MfDwVv5MVX1x5DOTJEnSepvW37mrqnOBc0c8F0mSJG0kP2VCkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6YriTJEnqiOFOkiSpI4Y7SZKkjhjuJEmSOmK4kyRJ6ojhTpIkqSOGO0mSpI4Y7iRJkjpiuJMkSeqI4U6SJKkjhjtJkqSOGO4kSZI6MrJwl2RpkhuTfHOo9oAkZyf5Tvu6c6snyQlJVia5NMmjhs5Z0o7/TpIlQ/VHJ7msnXNCkkw1hiRJ0tZglCt3pwAHjasdDZxTVQuAc9prgIOBBe1xJHAiDIIacAzwOOCxwDFDYe1E4AVD5x20jjEkSZK6N7JwV1XnAWvGlRcDp7bnpwJPG6qfVgPnAzsl2Q04EDi7qtZU1VrgbOCg1rZDVZ1fVQWcNq6vicaQJEnq3kxfc7drVV3fnv8A2LU93x24dui4Va02VX3VBPWpxriHJEcmWZFkxerVqzfg7UiSJG1eZu2GirbiVrM5RlWdVFWLqmrRvHnzRjkVSZKkGTHT4e6GtqVK+3pjq18H7Dl03B6tNlV9jwnqU40hSZLUvZkOd8uAsTtelwCfGqof1u6a3Re4pW2tngUckGTndiPFAcBZre3WJPu2u2QPG9fXRGNIkiR1b+6oOk5yOrAfsEuSVQzuen0LcGaSI4BrgGe1w5cDhwArgZ8AhwNU1ZokxwEXtuPeVFVjN2m8mMEdufcFPtseTDGGJElS90YW7qrqOZM07T/BsQW8ZJJ+lgJLJ6ivAB4+Qf3micaQJEnaGvgJFZIkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktSRWQl3Sa5OclmSS5KsaLUHJDk7yXfa151bPUlOSLIyyaVJHjXUz5J2/HeSLBmqP7r1v7Kdm5l/l5IkSTNvNlfu/riq9qmqRe310cA5VbUAOKe9BjgYWNAeRwInwiAMAscAjwMeCxwzFgjbMS8YOu+g0b8dSZKk2bc5bcsuBk5tz08FnjZUP60Gzgd2SrIbcCBwdlWtqaq1wNnAQa1th6o6v6oKOG2oL0mSpK7NVrgr4PNJLkpyZKvtWlXXt+c/AHZtz3cHrh06d1WrTVVfNUH9HpIcmWRFkhWrV6/emPcjSZK0WZg7S+M+oaquS/JrwNlJvj3cWFWVpEY9iao6CTgJYNGiRSMfT5IkadRmZeWuqq5rX28EPsHgmrkb2pYq7euN7fDrgD2HTt+j1aaq7zFBXZIkqXszHu6S3D/J9mPPgQOAbwLLgLE7XpcAn2rPlwGHtbtm9wVuadu3ZwEHJNm53UhxAHBWa7s1yb7tLtnDhvqSJEnq2mxsy+4KfKL9dZK5wIeq6nNJLgTOTHIEcA3wrHb8cuAQYCXwE+BwgKpak+Q44MJ23Juqak17/mLgFOC+wGfbQ5IkqXszHu6q6irgERPUbwb2n6BewEsm6WspsHSC+grg4Rs9WUmSpC3M5vSnUCRJkrSRDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHVk7mxPQJK06RxwxmtnewrSVunzh/7jbE/hTq7cSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw50kSVJHDHeSJEkdMdxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1JFuw12Sg5JcmWRlkqNnez6SJEkzoctwl2QO8E7gYGAh8JwkC2d3VpIkSaPXZbgDHgusrKqrquoXwBnA4lmekyRJ0sj1Gu52B64der2q1SRJkro2d7YnMJuSHAkc2V7+KMmVszkfbTF2AW6a7Ulow+Sk/z3bU5Am478tW7A85y0zPeRekzX0Gu6uA/Ycer1Hq91NVZ0EnDRTk1IfkqyoqkWzPQ9JffHfFm0qvW7LXggsSLJ3km2BQ4FlszwnSZKkkety5a6qbk/yUuAsYA6wtKoun+VpSZIkjVyX4Q6gqpYDy2d7HuqSW/mSRsF/W7RJpKpmew6SJEnaRHq95k6SJGmrZLiTJrGuj7BLcu8kH27tFySZPwvTlLSFSbI0yY1JvjlJe5Kc0P5tuTTJo2Z6jtqyGe6kCUzzI+yOANZW1UOAtwNvndlZStpCnQIcNEX7wcCC9jgSOHEG5qSOGO6kiU3nI+wWA6e25x8F9k+SGZyjpC1QVZ0HrJnikMXAaTVwPrBTkt1mZnbqgeFOmth0PsLuzmOq6nbgFuCBMzI7ST3zIzS1UQx3kiRJHTHcSRObzkfY3XlMkrnAjsDNMzI7ST2b1kdoSpMx3EkTm85H2C0DlrTnzwC+WP7hSEkbbxlwWLtrdl/glqq6frYnpS1Ht59QIW2MyT7CLsmbgBVVtQw4GXh/kpUMLo4+dPZmLGlLkeR0YD9glySrgGOAewFU1bsZfLrSIcBK4CfA4bMzU22p/IQKSZKkjrgtK0mS1BHDnSRJUkcMd5IkSR0x3EmSJHXEcCdJktQRw52kLU6SH417/fwk71jPPq5Ossum6m+KceYnee7Q6/2S3JLk60muTHJekqcMtb8oyWGbYmxJWyf/zp0kjUj75JL5wHOBDw01fbmqntKO2Qf4ZJKfVtU57e+cSdIGc+VOUleSPDXJBW1l7AtJdm31Byb5fJLLk7wPyDT7m5fkY0kubI/Ht/pjk3y1jfNfSR7a6s9PsizJF4FzgLcAT0xySZK/Hd9/VV0CvAl4aTv/2CSvas9fnuSKJJcmOaPV7p9kaZKvtbEXt/r8JF9OcnF7/EGr79ZWBy9J8s0kT2z1A9r8L07ykSTbbfA3XdJmxZU7SVui+ya5ZOj1A7jr4+G+AuxbVZXkr4G/A45i8CkAX6mqNyX5E+CIafb3b8Dbq+orSR7M4FNLHgZ8G3hi+zSTJwP/APx5O+dRwO9V1Zok+wGvGlqp22+C93Mx8OoJ6kcDe1fVz5Ps1GqvZ/BRd3/Val9L8gXgRuB/VdXPkiwATgcWMVg1PKuqjk8yB7hf245+A/DkqvpxktcAr2QQMiVt4Qx3krZEP62qfcZeJHk+gyADgw9Z/3CS3YBtge+2+h8CfwZQVZ9Jsnaa/T0ZWJjcudC3Q1vl2hE4tQWpon18VHN2Va1Zj/cz2SripcAHk3wS+GSrHQD86djqHnAf4MHA94F3tG3eO4Dfau0XAkuT3Av4ZFVdkuSPgIXAf7b3tS3w1fWYr6TNmOFOUm/+HXhbVS1rq2THbmR/2zBYCfzZcLHdcHFuVT09yXzgP4aaf7yeYzwS+NYE9T9hEEqfCrw+ye8yCIJ/XlVXjpvPscANwCPanH8GUFXnJfnD1tcpSd4GrGUQQJ+znvOUtAXwmjtJvdkRuK49XzJUP4/BFiVJDgZ2nmZ/nwdeNvairYyNH+f5U5x/G7D9ZI1Jfg94I/DOcfVtgD2r6lzgNW287RhsC78sbcktySOH5nN9Vf0KeB4wp7XvBdxQVe8F3sdgy/h84PFJHtKOuX+S30JSFwx3knpzLPCRJBcBNw3V/x74wySXM9ie/d40+3s5sKjd1HAF8KJW/yfgH5N8nal3QS4F7kjyjaEbKp449qdQGIS6l1fVOePOmwN8IMllwNeBE6rqh8BxDLaAL23v5bh2/LuAJUm+Afw2d60e7gd8o83z2cC/VdVqBoH09CSXMtiS/e1pfj8kbeZSVbM9B0mSJG0irtxJkiR1xHAnSZLUEcOdJElSRwx3kiRJHTHcSZIkdcRwJ0mS1BHDnSRJUkcMd5IkSR35//LFKVHXsWtPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the 'HadHeartDisease' column to a Pandas DataFrame\n",
    "pandas_df = reduced_df.select('HadHeartDisease').toPandas()\n",
    "\n",
    "# Plot the distribution of 'HadHeartDisease'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=pandas_df, x='HadHeartDisease', palette='viridis')\n",
    "plt.title('Distribution of HadHeartDisease')\n",
    "plt.xlabel('HadHeartDisease')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a084f3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 135:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|HadHeartDisease|count|\n",
      "+---------------+-----+\n",
      "|            0.0|25564|\n",
      "|            1.0|25459|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Separate the DataFrame into majority and minority classes\n",
    "majority_class = reduced_df.filter(F.col('HadHeartDisease') == 0)  \n",
    "minority_class = reduced_df.filter(F.col('HadHeartDisease') == 1)  \n",
    "\n",
    "# Get the size of the minority class\n",
    "minority_count = minority_class.count()\n",
    "\n",
    "# Downsample the majority class to match the size of the minority class\n",
    "majority_class_downsampled = majority_class.sample(False, float(minority_count) / majority_class.count())\n",
    "\n",
    "balanced_df = majority_class_downsampled.union(minority_class)\n",
    "\n",
    "# Verify the balance\n",
    "balanced_df.groupBy('HadHeartDisease').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b29ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:44:54 WARN DAGScheduler: Broadcasting large task binary with size 1002.1 KiB\n",
      "24/05/23 00:44:54 WARN DAGScheduler: Broadcasting large task binary with size 1005.2 KiB\n",
      "24/05/23 00:44:54 WARN DAGScheduler: Broadcasting large task binary with size 1005.6 KiB\n",
      "24/05/23 00:44:54 WARN DAGScheduler: Broadcasting large task binary with size 1006.2 KiB\n",
      "24/05/23 00:44:54 WARN DAGScheduler: Broadcasting large task binary with size 1007.5 KiB\n",
      "24/05/23 00:44:55 WARN DAGScheduler: Broadcasting large task binary with size 1009.9 KiB\n",
      "24/05/23 00:44:55 WARN DAGScheduler: Broadcasting large task binary with size 1012.8 KiB\n",
      "24/05/23 00:44:55 WARN DAGScheduler: Broadcasting large task binary with size 1013.3 KiB\n",
      "24/05/23 00:44:55 WARN DAGScheduler: Broadcasting large task binary with size 1013.9 KiB\n",
      "24/05/23 00:44:55 WARN DAGScheduler: Broadcasting large task binary with size 1015.1 KiB\n",
      "24/05/23 00:44:56 WARN DAGScheduler: Broadcasting large task binary with size 1017.6 KiB\n",
      "24/05/23 00:44:56 WARN DAGScheduler: Broadcasting large task binary with size 1020.8 KiB\n",
      "24/05/23 00:44:56 WARN DAGScheduler: Broadcasting large task binary with size 1021.2 KiB\n",
      "24/05/23 00:44:56 WARN DAGScheduler: Broadcasting large task binary with size 1021.9 KiB\n",
      "24/05/23 00:44:57 WARN DAGScheduler: Broadcasting large task binary with size 1023.1 KiB\n",
      "24/05/23 00:44:57 WARN DAGScheduler: Broadcasting large task binary with size 1025.4 KiB\n",
      "24/05/23 00:44:57 WARN DAGScheduler: Broadcasting large task binary with size 1028.4 KiB\n",
      "24/05/23 00:44:57 WARN DAGScheduler: Broadcasting large task binary with size 1028.9 KiB\n",
      "24/05/23 00:44:57 WARN DAGScheduler: Broadcasting large task binary with size 1029.5 KiB\n",
      "24/05/23 00:44:57 WARN DAGScheduler: Broadcasting large task binary with size 1030.7 KiB\n",
      "24/05/23 00:44:58 WARN DAGScheduler: Broadcasting large task binary with size 1033.1 KiB\n",
      "24/05/23 00:44:59 WARN DAGScheduler: Broadcasting large task binary with size 1024.8 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gradient-Boosted Tree Classifier: 0.7492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:53:44 WARN DAGScheduler: Broadcasting large task binary with size 1016.3 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Gradient-Boosted Tree Classifier: 0.8236\n",
      "Feature Importance:\n",
      "AgeCategory: 0.2228\n",
      "BMI: 0.0947\n",
      "SleepHours: 0.0745\n",
      "HadArthritis: 0.0686\n",
      "HadKidneyDisease: 0.0652\n",
      "PneumoVaxEver: 0.0573\n",
      "Sex: 0.0547\n",
      "HadStroke: 0.0507\n",
      "Smoked: 0.0487\n",
      "RaceEth: 0.0469\n",
      "Diabetic: 0.0445\n",
      "HadCOPD: 0.0376\n",
      "AlcoholDrinkers: 0.0349\n",
      "HadDepressiveDisorder: 0.0286\n",
      "PhysicalActivities: 0.0249\n",
      "HadAsthma: 0.0131\n",
      "HIVTesting: 0.0121\n",
      "FluVaxLast12: 0.0118\n",
      "HadSkinCancer: 0.0083\n"
     ]
    }
   ],
   "source": [
    "#Iteration 3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "# Combine all features into a single vector column\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "model_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = model_df.randomSplit([0.7, 0.3], seed=1234)\n",
    "\n",
    "# Fit the Gradient-Boosted Tree Classifier\n",
    "gbt = GBTClassifier(labelCol='HadHeartDisease', featuresCol='features', maxIter=100)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of Gradient-Boosted Tree Classifier: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model's AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of Gradient-Boosted Tree Classifier: {auc:.4f}\")\n",
    "\n",
    "# Extract Feature Importances\n",
    "importances = gbt_model.featureImportances\n",
    "feature_importances = sorted(zip(importances, feature_columns), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Display Feature Importances in descending order\n",
    "print(\"Feature Importance:\")\n",
    "for importance, feature in feature_importances:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a0692e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 00:54:37 WARN BlockManager: Asked to remove block broadcast_1686_piece0, which does not exist\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.7479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Logistic Regression: 0.8247\n",
      "Feature Coefficients:\n",
      "SleepHours: -0.0487\n",
      "BMI: 0.0156\n",
      "Sex: 0.8522\n",
      "PhysicalActivities: 0.2090\n",
      "HadStroke: 1.2993\n",
      "HadAsthma: 0.1698\n",
      "HadSkinCancer: 0.1034\n",
      "HadCOPD: 0.6708\n",
      "HadDepressiveDisorder: 0.3765\n",
      "HadKidneyDisease: 0.7153\n",
      "HadArthritis: 0.3560\n",
      "AlcoholDrinkers: 0.3334\n",
      "HIVTesting: 0.1225\n",
      "FluVaxLast12: 0.0336\n",
      "PneumoVaxEver: 0.2847\n",
      "Smoked: 0.3056\n",
      "Diabetic: 0.6210\n",
      "RaceEth: 0.0135\n",
      "AgeCategory: 0.2767\n",
      "Intercept: -4.0841\n"
     ]
    }
   ],
   "source": [
    "#Iteration 3\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "model_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = model_df.randomSplit([0.7, 0.3], seed=1234)\n",
    "\n",
    "# Fit the Logistic Regression model\n",
    "lr = LogisticRegression(labelCol='HadHeartDisease', featuresCol='features', maxIter=100)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of Logistic Regression: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model's AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of Logistic Regression: {auc:.4f}\")\n",
    "\n",
    "# Extract Feature Coefficients\n",
    "coefficients = lr_model.coefficients.toArray()\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "# Display Feature Coefficients\n",
    "print(\"Feature Coefficients:\")\n",
    "for feature, coefficient in zip(feature_columns, coefficients):\n",
    "    print(f\"{feature}: {coefficient:.4f}\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6791e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
