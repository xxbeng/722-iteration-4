{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c5c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/20 02:03:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('heart-disease-mining-clean') \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6cc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dirt= spark.read.csv('Datasets/heart_2022_with_nans - with dirt.csv', header=True, inferSchema=True)\n",
    "#for column in df_dirt.columns:\n",
    "    #null_count = df_dirt.filter(df_dirt[column].isNull()).count()\n",
    "    #print(f\"Column '{column}': {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1032f3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        SmokerStatus| count|\n",
      "+--------------------+------+\n",
      "|        Never smoked|245955|\n",
      "|       Former smoker|113769|\n",
      "|Current smoker - ...| 36003|\n",
      "|                null| 35462|\n",
      "|Current smoker - ...| 13938|\n",
      "|     Previous smoker|     5|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# use groupby to calculate value counts for 'SmokerStatus'\n",
    "smoker_status_counts = df_dirt.groupBy('SmokerStatus').count().orderBy('count', ascending=False)\n",
    "smoker_status_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f44aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    AgeCategory|count|\n",
      "+---------------+-----+\n",
      "|   Age 65 to 69|47098|\n",
      "|   Age 60 to 64|44511|\n",
      "|   Age 70 to 74|43472|\n",
      "|   Age 55 to 59|36821|\n",
      "|Age 80 or older|36250|\n",
      "|   Age 50 to 54|33644|\n",
      "|   Age 75 to 79|32516|\n",
      "|   Age 40 to 44|29942|\n",
      "|   Age 45 to 49|28531|\n",
      "|   Age 35 to 39|28526|\n",
      "|   Age 18 to 24|26941|\n",
      "|   Age 30 to 34|25806|\n",
      "|   Age 25 to 29|21989|\n",
      "|           null| 9079|\n",
      "|             78|    1|\n",
      "|             67|    1|\n",
      "|             76|    1|\n",
      "|             88|    1|\n",
      "|             34|    1|\n",
      "|             26|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Age_counts = df_dirt.groupBy('AgeCategory').count().orderBy('count', ascending=False)\n",
    "Age_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc37311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|WeightInKilograms|\n",
      "+-------+-----------------+\n",
      "|  count|           403054|\n",
      "|   mean|83.10065152559405|\n",
      "| stddev|27.15749401703654|\n",
      "|    min|            22.68|\n",
      "|    max|          10659.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dirt.describe('WeightInKilograms').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246c85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "columns_to_exclude = [\n",
    "    'State', 'LastCheckupTime', 'ChestScan', 'GeneralHealth',\n",
    "    'PhysicalHealthDays', 'MentalHealthDays', 'RemovedTeeth',\n",
    "    'DeafOrHardOfHearing', 'BlindOrVisionDifficulty',\n",
    "    'DifficultyConcentrating', 'DifficultyWalking',\n",
    "    'DifficultyDressingBathing', 'DifficultyErrands',\n",
    "    'HeightInMeters', 'WeightInKilograms'  \n",
    "]\n",
    "df_03 = df_dirt.drop(*columns_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae9bfa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex',\n",
       " 'PhysicalActivities',\n",
       " 'SleepHours',\n",
       " 'HadHeartAttack',\n",
       " 'HadAngina',\n",
       " 'HadStroke',\n",
       " 'HadAsthma',\n",
       " 'HadSkinCancer',\n",
       " 'HadCOPD',\n",
       " 'HadDepressiveDisorder',\n",
       " 'HadKidneyDisease',\n",
       " 'HadArthritis',\n",
       " 'HadDiabetes',\n",
       " 'SmokerStatus',\n",
       " 'ECigaretteUsage',\n",
       " 'RaceEthnicityCategory',\n",
       " 'AgeCategory',\n",
       " 'BMI',\n",
       " 'AlcoholDrinkers',\n",
       " 'HIVTesting',\n",
       " 'FluVaxLast12',\n",
       " 'PneumoVaxEver',\n",
       " 'TetanusLast10Tdap',\n",
       " 'HighRiskLastYear',\n",
       " 'CovidPos']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_03.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d325bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 1093\n",
      "SleepHours: 5453\n",
      "HadHeartAttack: 3065\n",
      "HadAngina: 4405\n",
      "HadStroke: 1557\n",
      "HadAsthma: 1773\n",
      "HadSkinCancer: 3143\n",
      "HadCOPD: 2219\n",
      "HadDepressiveDisorder: 2812\n",
      "HadKidneyDisease: 1926\n",
      "HadArthritis: 2633\n",
      "HadDiabetes: 1087\n",
      "SmokerStatus: 35462\n",
      "ECigaretteUsage: 35660\n",
      "RaceEthnicityCategory: 14057\n",
      "AgeCategory: 9079\n",
      "BMI: 48806\n",
      "AlcoholDrinkers: 46574\n",
      "HIVTesting: 66127\n",
      "FluVaxLast12: 47121\n",
      "PneumoVaxEver: 77040\n",
      "TetanusLast10Tdap: 82516\n",
      "HighRiskLastYear: 50623\n",
      "CovidPos: 50764\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a list of columns with their respective null counts using isnull\n",
    "null_counts = df_03.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8573a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0.00%\n",
      "PhysicalActivities: 0.25%\n",
      "SleepHours: 1.23%\n",
      "HadHeartAttack: 0.69%\n",
      "HadAngina: 0.99%\n",
      "HadStroke: 0.35%\n",
      "HadAsthma: 0.40%\n",
      "HadSkinCancer: 0.71%\n",
      "HadCOPD: 0.50%\n",
      "HadDepressiveDisorder: 0.63%\n",
      "HadKidneyDisease: 0.43%\n",
      "HadArthritis: 0.59%\n",
      "HadDiabetes: 0.24%\n",
      "SmokerStatus: 7.97%\n",
      "ECigaretteUsage: 8.01%\n",
      "RaceEthnicityCategory: 3.16%\n",
      "AgeCategory: 2.04%\n",
      "BMI: 10.96%\n",
      "AlcoholDrinkers: 10.46%\n",
      "HIVTesting: 14.86%\n",
      "FluVaxLast12: 10.59%\n",
      "PneumoVaxEver: 17.31%\n",
      "TetanusLast10Tdap: 18.54%\n",
      "HighRiskLastYear: 11.37%\n",
      "CovidPos: 11.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the total number of rows in the DataFrame\n",
    "total_count = df_03.count()\n",
    "\n",
    "# Calculate the number of null values and the percentage of nulls for each column\n",
    "null_percentage = df_03.select([\n",
    "    ((F.count(F.when(F.col(c).isNull(), c)) / total_count) * 100).alias(c)\n",
    "    for c in df_03.columns\n",
    "])\n",
    "\n",
    "null_percentage_dict = null_percentage.collect()[0].asDict()\n",
    "\n",
    "for column, percentage in null_percentage_dict.items():\n",
    "    print(f\"{column}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9604e34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 0\n",
      "SleepHours: 2221\n",
      "HadHeartAttack: 0\n",
      "HadAngina: 0\n",
      "HadStroke: 0\n",
      "HadAsthma: 0\n",
      "HadSkinCancer: 0\n",
      "HadCOPD: 0\n",
      "HadDepressiveDisorder: 0\n",
      "HadKidneyDisease: 0\n",
      "HadArthritis: 0\n",
      "HadDiabetes: 0\n",
      "SmokerStatus: 0\n",
      "ECigaretteUsage: 0\n",
      "RaceEthnicityCategory: 0\n",
      "AgeCategory: 0\n",
      "BMI: 15757\n",
      "AlcoholDrinkers: 0\n",
      "HIVTesting: 0\n",
      "FluVaxLast12: 0\n",
      "PneumoVaxEver: 0\n",
      "TetanusLast10Tdap: 0\n",
      "HighRiskLastYear: 0\n",
      "CovidPos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# List of categorical columns to check for null values\n",
    "categorical_columns = [\n",
    "    'Sex', 'PhysicalActivities', 'HadHeartAttack', 'HadAngina', 'HadStroke', 'HadAsthma', \n",
    "    'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder', 'HadKidneyDisease', 'HadArthritis', \n",
    "    'HadDiabetes', 'SmokerStatus', 'ECigaretteUsage', 'RaceEthnicityCategory', 'AgeCategory', \n",
    "    'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver', 'TetanusLast10Tdap', \n",
    "    'HighRiskLastYear', 'CovidPos'\n",
    "]\n",
    "\n",
    "df_03_cleaned_01= df_03.na.drop(subset=categorical_columns)\n",
    "\n",
    "# Count null values in each column after dropping rows\n",
    "null_counts = df_03_cleaned_01.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03_cleaned_01.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "# Print the null counts for each column\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dcb016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 0\n",
      "SleepHours: 0\n",
      "HadHeartAttack: 0\n",
      "HadAngina: 0\n",
      "HadStroke: 0\n",
      "HadAsthma: 0\n",
      "HadSkinCancer: 0\n",
      "HadCOPD: 0\n",
      "HadDepressiveDisorder: 0\n",
      "HadKidneyDisease: 0\n",
      "HadArthritis: 0\n",
      "HadDiabetes: 0\n",
      "SmokerStatus: 0\n",
      "ECigaretteUsage: 0\n",
      "RaceEthnicityCategory: 0\n",
      "AgeCategory: 0\n",
      "BMI: 0\n",
      "AlcoholDrinkers: 0\n",
      "HIVTesting: 0\n",
      "FluVaxLast12: 0\n",
      "PneumoVaxEver: 0\n",
      "TetanusLast10Tdap: 0\n",
      "HighRiskLastYear: 0\n",
      "CovidPos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Calculate the mean of BMI and SleepHours\n",
    "bmi_mean = df_03_cleaned_01.select(mean('BMI')).first()[0]\n",
    "sleep_hours_mean = df_03_cleaned_01.select(mean('SleepHours')).first()[0]\n",
    "\n",
    "# Replace null values with the mean\n",
    "df_03_cleaned_02 = df_03_cleaned_01.fillna({'BMI': bmi_mean, 'SleepHours': sleep_hours_mean})\n",
    "\n",
    "# Verify the changes by counting null values again\n",
    "null_counts = df_03_cleaned_02.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03_cleaned_02.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "# Print the null counts for each column\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5595d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns before filtering: 288444 rows x 25 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns after filtering: 285788 rows x 25 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bmi_lower_bound = df_03_cleaned_02.approxQuantile(\"BMI\", [0.0025], 0.0)[0]\n",
    "bmi_upper_bound = df_03_cleaned_02.approxQuantile(\"BMI\", [0.9975], 0.0)[0]\n",
    "\n",
    "sleep_lower_bound = df_03_cleaned_02.approxQuantile(\"SleepHours\", [0.0025], 0.0)[0]\n",
    "sleep_upper_bound = df_03_cleaned_02.approxQuantile(\"SleepHours\", [0.9975], 0.0)[0]\n",
    "\n",
    "# Filter the dataframe based on the calculated bounds\n",
    "df_03_cleaned_03 = df_03_cleaned_02.filter(\n",
    "    (F.col('BMI') >= bmi_lower_bound) & (F.col('BMI') <= bmi_upper_bound) &\n",
    "    (F.col('SleepHours') >= sleep_lower_bound) & (F.col('SleepHours') <= sleep_upper_bound)\n",
    ")\n",
    "\n",
    "# Show the number of rows and columns after filtering\n",
    "print(f\"Number of rows and columns before filtering: {df_03_cleaned_02.count()} rows x {len(df_03_cleaned_02.columns)} columns\")\n",
    "print(f\"Number of rows and columns after filtering: {df_03_cleaned_03.count()} rows x {len(df_03_cleaned_03.columns)} columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3131ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               BMI|\n",
      "+-------+------------------+\n",
      "|  count|            285788|\n",
      "|   mean|28.596861735345282|\n",
      "| stddev| 6.059249795657394|\n",
      "|    min|             16.24|\n",
      "|    max|             57.39|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_03_cleaned_03.describe('BMI').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed7e4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|    AgeCategory|\n",
      "+---------------+\n",
      "|   Age 45 to 49|\n",
      "|   Age 25 to 29|\n",
      "|   Age 70 to 74|\n",
      "|   Age 55 to 59|\n",
      "|   Age 18 to 24|\n",
      "|   Age 60 to 64|\n",
      "|   Age 50 to 54|\n",
      "|   Age 35 to 39|\n",
      "|   Age 30 to 34|\n",
      "|Age 80 or older|\n",
      "|   Age 40 to 44|\n",
      "|   Age 75 to 79|\n",
      "|   Age 65 to 69|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to reclassify numerical ages to age categories\n",
    "def reclassify_age(age):\n",
    "    if age <= 25:\n",
    "        return 'Age 18 to 24'\n",
    "    elif 25 < age <= 30:\n",
    "        return 'Age 25 to 29'\n",
    "    elif 30 < age <= 35:\n",
    "        return 'Age 30 to 34'\n",
    "    elif 35 < age <= 40:\n",
    "        return 'Age 35 to 39'\n",
    "    elif 40 < age <= 45:\n",
    "        return 'Age 40 to 44'\n",
    "    elif 45 < age <= 50:\n",
    "        return 'Age 45 to 49'\n",
    "    elif 50 < age <= 55:\n",
    "        return 'Age 50 to 54'\n",
    "    elif 55 < age <= 60:\n",
    "        return 'Age 55 to 59'\n",
    "    elif 60 < age <= 65:\n",
    "        return 'Age 60 to 64'\n",
    "    elif 65 < age <= 70:\n",
    "        return 'Age 65 to 69'\n",
    "    elif 70 < age <= 75:\n",
    "        return 'Age 70 to 74'\n",
    "    elif 75 < age <= 80:\n",
    "        return 'Age 75 to 79'\n",
    "    else:\n",
    "        return 'Age 80 or older'\n",
    "\n",
    "# UDF to apply the reclassification function\n",
    "reclassify_age_udf = F.udf(lambda x: reclassify_age(int(x)) if x.isdigit() else x, StringType())\n",
    "df_03_cleaned_04 = df_03_cleaned_03.withColumn('AgeCategory', reclassify_age_udf(F.col('AgeCategory')))\n",
    "\n",
    "# Show the reclassified age category\n",
    "df_03_cleaned_04.select('AgeCategory').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb0c7c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        SmokerStatus|\n",
      "+--------------------+\n",
      "|        Never smoked|\n",
      "|Current smoker - ...|\n",
      "|       Former smoker|\n",
      "|Current smoker - ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Function to reclassify smoker status\n",
    "def reclassify_smoker(status):\n",
    "    if status == 'Previous smoker':\n",
    "        return 'Former smoker'\n",
    "    else:\n",
    "        return status\n",
    "\n",
    "# UDF to apply the reclassification function\n",
    "reclassify_smoker_udf = F.udf(lambda x: reclassify_smoker(x), StringType())\n",
    "\n",
    "# Apply the UDF to reclassify smoker status\n",
    "df_03_cleaned_05 = df_03_cleaned_04.withColumn('SmokerStatus', reclassify_smoker_udf(F.col('SmokerStatus')))\n",
    "\n",
    "# Show the reclassified DataFrame\n",
    "df_03_cleaned_05.select('SmokerStatus').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cb61ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------------+\n",
      "|HadHeartAttack|HadAngina|HadHeartDisease|\n",
      "+--------------+---------+---------------+\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|           Yes|       No|            Yes|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|           Yes|      Yes|            Yes|\n",
      "|            No|       No|             No|\n",
      "|            No|      Yes|            Yes|\n",
      "|            No|       No|             No|\n",
      "|           Yes|      Yes|            Yes|\n",
      "+--------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 51:>                                                         (0 + 1) / 1]\r",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Function to determine if a respondent had heart disease\n",
    "def had_heart_disease(had_heart_attack, had_angina):\n",
    "    return 'Yes' if had_heart_attack == 'Yes' or had_angina == 'Yes' else 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_heart_disease_udf = F.udf(lambda x, y: had_heart_disease(x, y), StringType())\n",
    "\n",
    "# Apply the UDF to create the HadHeartDisease column\n",
    "df_03_cleaned_06 = df_03_cleaned_05.withColumn('HadHeartDisease', had_heart_disease_udf(F.col('HadHeartAttack'), F.col('HadAngina')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_06.select('HadHeartAttack', 'HadAngina', 'HadHeartDisease').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ab572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|     ECigaretteUsage|EcigaFlag|\n",
      "+--------------------+---------+\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 52:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition two: EcigaFlag column\n",
    "\n",
    "# Function to determine the e-cigarette usage flag\n",
    "def ecig_new(response):\n",
    "    if response in [\"Use them every day\", \"Use them some days\"]:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "ecig_new_udf = F.udf(lambda x: ecig_new(x), StringType())\n",
    "\n",
    "# Apply the UDF to create the EcigaFlag column\n",
    "df_03_cleaned_07 = df_03_cleaned_06.withColumn('EcigaFlag', ecig_new_udf(F.col('ECigaretteUsage')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_07.select('ECigaretteUsage', 'EcigaFlag').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be6dc244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        SmokerStatus|Smoked|\n",
      "+--------------------+------+\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|Current smoker - ...|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|Current smoker - ...|   Yes|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition three: Smoked column\n",
    "# Function to determine if the person has smoked\n",
    "def smoker_new(status):\n",
    "    if status == \"Never smoked\":\n",
    "        return \"No\"\n",
    "    else:\n",
    "        return \"Yes\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "smoker_new_udf = F.udf(lambda x: smoker_new(x), StringType())\n",
    "\n",
    "# Apply the UDF to create the Smoked column\n",
    "df_03_cleaned_08 = df_03_cleaned_07.withColumn('Smoked', smoker_new_udf(F.col('SmokerStatus')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_08.select('SmokerStatus', 'Smoked').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3530475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|HadDiabetes|Diabetic|\n",
      "+-----------+--------+\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition four: Diabetic column\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "# Function to determine if the person is diabetic\n",
    "def diabetes_new(status):\n",
    "    if status == 'Yes' or status == 'Yes, but only during pregnancy (female)':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "diabetes_new_udf = udf(diabetes_new, StringType())\n",
    "\n",
    "# Apply the UDF to create the Diabetic column\n",
    "df_03_cleaned_09 = df_03_cleaned_08.withColumn('Diabetic', diabetes_new_udf(F.col('HadDiabetes')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_09.select('HadDiabetes', 'Diabetic').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb94c244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 55:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Diabetic|\n",
      "+--------+\n",
      "|      No|\n",
      "|     Yes|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_03_cleaned_09.select('Diabetic').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3e443a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|CovidPos|HadCovid|\n",
      "+--------+--------+\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|     Yes|     Yes|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|     Yes|     Yes|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to determine if the person had Covid\n",
    "def had_covid(status):\n",
    "    if status == 'Yes' or status == 'Tested positive using home test without a health professional':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_covid_udf = udf(had_covid, StringType())\n",
    "\n",
    "# Apply the UDF to create the HadCovid column\n",
    "df_03_cleaned_10 = df_03_cleaned_09.withColumn('HadCovid', had_covid_udf(F.col('CovidPos')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_10.select('CovidPos', 'HadCovid').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7123c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|   TetanusLast10Tdap|HadTetanus|\n",
      "+--------------------+----------+\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|  Yes, received Tdap|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to determine if the person had tetanus\n",
    "def had_tetanus(status):\n",
    "    if status == 'Yes, received tetanus shot but not sure what type' or status == 'Yes, received Tdap':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_tetanus_udf = udf(had_tetanus, StringType())\n",
    "\n",
    "# Apply the UDF to create the HadTetanus column\n",
    "df_03_cleaned_11 = df_03_cleaned_10.withColumn('HadTetanus', had_tetanus_udf(F.col('TetanusLast10Tdap')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_11.select('TetanusLast10Tdap', 'HadTetanus').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b19a66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|RaceEthnicityCategory|RaceEth|\n",
      "+---------------------+-------+\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "+---------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to reclassify race and ethnicity\n",
    "def race_ethnicity(category):\n",
    "    if category == 'White only, Non-Hispanic':\n",
    "        return 'White'\n",
    "    elif category == 'Black only, Non-Hispanic':\n",
    "        return 'Black'\n",
    "    elif category == 'Other race only, Non-Hispanic':\n",
    "        return 'Other'\n",
    "    elif category == 'Hispanic':\n",
    "        return 'Hispanic'\n",
    "    elif category == 'Multiracial, Non-Hispanic':\n",
    "        return 'Multiracial'\n",
    "    else:\n",
    "        return category\n",
    "\n",
    "# Register the function as a UDF\n",
    "race_ethnicity_udf = udf(race_ethnicity, StringType())\n",
    "\n",
    "# Apply the UDF to create the RaceEth column\n",
    "df_03_cleaned_12 = df_03_cleaned_11.withColumn('RaceEth', race_ethnicity_udf(F.col('RaceEthnicityCategory')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_12.select('RaceEthnicityCategory', 'RaceEth').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c1bd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- PhysicalActivities: string (nullable = true)\n",
      " |-- SleepHours: integer (nullable = true)\n",
      " |-- HadStroke: string (nullable = true)\n",
      " |-- HadAsthma: string (nullable = true)\n",
      " |-- HadSkinCancer: string (nullable = true)\n",
      " |-- HadCOPD: string (nullable = true)\n",
      " |-- HadDepressiveDisorder: string (nullable = true)\n",
      " |-- HadKidneyDisease: string (nullable = true)\n",
      " |-- HadArthritis: string (nullable = true)\n",
      " |-- AgeCategory: string (nullable = true)\n",
      " |-- BMI: double (nullable = false)\n",
      " |-- AlcoholDrinkers: string (nullable = true)\n",
      " |-- HIVTesting: string (nullable = true)\n",
      " |-- FluVaxLast12: string (nullable = true)\n",
      " |-- PneumoVaxEver: string (nullable = true)\n",
      " |-- HighRiskLastYear: string (nullable = true)\n",
      " |-- HadHeartDisease: string (nullable = true)\n",
      " |-- EcigaFlag: string (nullable = true)\n",
      " |-- Smoked: string (nullable = true)\n",
      " |-- Diabetic: string (nullable = true)\n",
      " |-- HadCovid: string (nullable = true)\n",
      " |-- HadTetanus: string (nullable = true)\n",
      " |-- RaceEth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.5 Data formatting\n",
    "cleaned_df = df_03_cleaned_12\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"ECigaretteUsage\", \"SmokerStatus\", \n",
    "                   \"HadHeartAttack\", \"HadAngina\", \"HadDiabetes\", \"CovidPos\", \"TetanusLast10Tdap\", \"RaceEthnicityCategory\"]\n",
    "\n",
    "# Drop the columns\n",
    "formatted_df_01 = cleaned_df.drop(*columns_to_drop)\n",
    "\n",
    "# Show the schema to verify columns are dropped\n",
    "formatted_df_01.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a4791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SleepHours: integer (nullable = true)\n",
      " |-- BMI: double (nullable = false)\n",
      " |-- Sex: double (nullable = false)\n",
      " |-- PhysicalActivities: double (nullable = false)\n",
      " |-- HadStroke: double (nullable = false)\n",
      " |-- HadAsthma: double (nullable = false)\n",
      " |-- HadSkinCancer: double (nullable = false)\n",
      " |-- HadCOPD: double (nullable = false)\n",
      " |-- HadDepressiveDisorder: double (nullable = false)\n",
      " |-- HadKidneyDisease: double (nullable = false)\n",
      " |-- HadArthritis: double (nullable = false)\n",
      " |-- AlcoholDrinkers: double (nullable = false)\n",
      " |-- HIVTesting: double (nullable = false)\n",
      " |-- FluVaxLast12: double (nullable = false)\n",
      " |-- PneumoVaxEver: double (nullable = false)\n",
      " |-- HighRiskLastYear: double (nullable = false)\n",
      " |-- HadHeartDisease: double (nullable = false)\n",
      " |-- EcigaFlag: double (nullable = false)\n",
      " |-- Smoked: double (nullable = false)\n",
      " |-- Diabetic: double (nullable = false)\n",
      " |-- HadCovid: double (nullable = false)\n",
      " |-- HadTetanus: double (nullable = false)\n",
      " |-- AgeCategory: double (nullable = false)\n",
      " |-- RaceEth: double (nullable = false)\n",
      "\n",
      "+----------+------------------+---+------------------+---------+---------+-------------+-------+---------------------+----------------+------------+---------------+----------+------------+-------------+----------------+---------------+---------+------+--------+--------+----------+-----------+-------+\n",
      "|SleepHours|               BMI|Sex|PhysicalActivities|HadStroke|HadAsthma|HadSkinCancer|HadCOPD|HadDepressiveDisorder|HadKidneyDisease|HadArthritis|AlcoholDrinkers|HIVTesting|FluVaxLast12|PneumoVaxEver|HighRiskLastYear|HadHeartDisease|EcigaFlag|Smoked|Diabetic|HadCovid|HadTetanus|AgeCategory|RaceEth|\n",
      "+----------+------------------+---+------------------+---------+---------+-------------+-------+---------------------+----------------+------------+---------------+----------+------------+-------------+----------------+---------------+---------+------+--------+--------+----------+-----------+-------+\n",
      "|         8|28.652459119794525|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         0.0|          0.0|             0.0|            0.0|      0.0|   0.0|     1.0|     0.0|       0.0|        5.0|    0.0|\n",
      "|         6|             26.57|0.0|               1.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         1.0|          0.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       1.0|        5.0|    0.0|\n",
      "|         9|             21.77|0.0|               0.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            0.0|       0.0|         1.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       1.0|        7.0|    0.0|\n",
      "|         7|             26.08|1.0|               1.0|      1.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         1.0|          1.0|             0.0|            1.0|      0.0|   0.0|     1.0|     0.0|       1.0|        5.0|    0.0|\n",
      "|         7|             22.96|0.0|               0.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            0.0|       0.0|         1.0|          0.0|             0.0|            0.0|      0.0|   1.0|     0.0|     0.0|       1.0|        5.0|    2.0|\n",
      "|         8|             27.81|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|        5.0|    0.0|\n",
      "|         6|28.652459119794525|0.0|               0.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       1.0|         1.0|          0.0|             0.0|            0.0|      0.0|   1.0|     0.0|     0.0|       0.0|        6.0|    0.0|\n",
      "|         8|             29.23|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|        5.0|    0.0|\n",
      "|         6|             23.21|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         0.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|        5.0|    0.0|\n",
      "|         6|             28.59|1.0|               1.0|      0.0|      1.0|          0.0|    0.0|                  0.0|             0.0|         1.0|            0.0|       1.0|         0.0|          0.0|             0.0|            0.0|      0.0|   1.0|     0.0|     0.0|       1.0|        3.0|    2.0|\n",
      "|         8|             25.34|0.0|               0.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         1.0|            0.0|       0.0|         1.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|        5.0|    0.0|\n",
      "|         6|             32.28|0.0|               0.0|      0.0|      1.0|          1.0|    1.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   1.0|     1.0|     1.0|       1.0|        5.0|    0.0|\n",
      "|         8|             44.59|0.0|               1.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             1.0|         1.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       1.0|        5.0|    0.0|\n",
      "|         8|             24.34|1.0|               0.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         0.0|          0.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|        0.0|    0.0|\n",
      "|         6|             21.63|0.0|               0.0|      0.0|      0.0|          1.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         1.0|          1.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       0.0|        6.0|    0.0|\n",
      "|         8|             37.45|0.0|               0.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         1.0|            0.0|       0.0|         0.0|          1.0|             0.0|            1.0|      0.0|   1.0|     1.0|     1.0|       0.0|        0.0|    2.0|\n",
      "|         8|             31.09|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  1.0|             0.0|         1.0|            1.0|       0.0|         0.0|          0.0|             0.0|            0.0|      0.0|   0.0|     0.0|     0.0|       1.0|        6.0|    0.0|\n",
      "|         8|             22.14|0.0|               1.0|      0.0|      0.0|          0.0|    1.0|                  0.0|             1.0|         0.0|            1.0|       0.0|         0.0|          0.0|             0.0|            1.0|      0.0|   1.0|     0.0|     0.0|       1.0|        0.0|    0.0|\n",
      "|         8|             29.12|0.0|               1.0|      0.0|      0.0|          0.0|    0.0|                  0.0|             0.0|         1.0|            1.0|       0.0|         0.0|          1.0|             0.0|            0.0|      0.0|   1.0|     1.0|     0.0|       1.0|        2.0|    0.0|\n",
      "|         4|             24.39|1.0|               0.0|      1.0|      0.0|          1.0|    0.0|                  1.0|             1.0|         1.0|            0.0|       1.0|         0.0|          1.0|             0.0|            1.0|      0.0|   0.0|     0.0|     0.0|       0.0|        6.0|    0.0|\n",
      "+----------+------------------+---+------------------+---------+---------+-------------+-------+---------------------+----------------+------------+---------------+----------+------------+-------------+----------------+---------------+---------+------+--------+--------+----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Define all categorical columns, including binary ones\n",
    "categorical_columns = [\n",
    "    'Sex', 'PhysicalActivities', 'HadStroke', 'HadAsthma', 'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder',\n",
    "    'HadKidneyDisease', 'HadArthritis', 'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver',\n",
    "    'HighRiskLastYear', 'HadHeartDisease', 'EcigaFlag', 'Smoked', 'Diabetic', 'HadCovid', 'HadTetanus',\n",
    "    'AgeCategory', 'RaceEth'\n",
    "]\n",
    "\n",
    "# Apply StringIndexer for all categorical columns\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_Index\") for column in categorical_columns]\n",
    "\n",
    "# Fit the StringIndexer models and transform the data\n",
    "for indexer in indexers:\n",
    "    formatted_df_01 = indexer.fit(formatted_df_01).transform(formatted_df_01)\n",
    "\n",
    "# Drop the original categorical columns and rename the indexed columns\n",
    "for column in categorical_columns:\n",
    "    formatted_df_01 = formatted_df_01.drop(column).withColumnRenamed(column + \"_Index\", column)\n",
    "\n",
    "# Show the schema to confirm changes\n",
    "formatted_df_01.printSchema()\n",
    "\n",
    "# Verify the first few rows to confirm changes\n",
    "formatted_df_01.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad441c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 128:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|HadHeartDisease| count|\n",
      "+---------------+------+\n",
      "|            0.0|260329|\n",
      "|            1.0| 25459|\n",
      "+---------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "formatted_df_01.groupBy('HadHeartDisease').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0013d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "HadStroke: 0.2547\n",
      "AgeCategory: 0.2156\n",
      "HadCOPD: 0.1520\n",
      "PneumoVaxEver: 0.1068\n",
      "Diabetic: 0.0893\n",
      "HadKidneyDisease: 0.0749\n",
      "HadArthritis: 0.0514\n",
      "Sex: 0.0191\n",
      "Smoked: 0.0148\n",
      "AlcoholDrinkers: 0.0077\n",
      "SleepHours: 0.0040\n",
      "HadSkinCancer: 0.0036\n",
      "PhysicalActivities: 0.0024\n",
      "BMI: 0.0008\n",
      "HadAsthma: 0.0007\n",
      "RaceEth: 0.0007\n",
      "HadDepressiveDisorder: 0.0005\n",
      "HIVTesting: 0.0003\n",
      "FluVaxLast12: 0.0002\n",
      "EcigaFlag: 0.0001\n",
      "HighRiskLastYear: 0.0001\n",
      "HadCovid: 0.0001\n",
      "HadTetanus: 0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "formatted_df = formatted_df_01\n",
    "# Combine all features into a single vector column\n",
    "feature_columns = [col for col in formatted_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "formatted_df = assembler.transform(formatted_df)\n",
    "\n",
    "# Fit the RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol='HadHeartDisease', featuresCol='features', numTrees=100)\n",
    "rf_model = rf.fit(formatted_df)\n",
    "\n",
    "# Extract Feature Importances\n",
    "importances = rf_model.featureImportances\n",
    "feature_importances = sorted(zip(importances, feature_columns), reverse=True)\n",
    "\n",
    "# Display Feature Importances in descending order\n",
    "print(\"Feature Importance:\")\n",
    "for importance, feature in feature_importances:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "277e1523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SleepHours: integer (nullable = true)\n",
      " |-- BMI: double (nullable = false)\n",
      " |-- Sex: double (nullable = false)\n",
      " |-- PhysicalActivities: double (nullable = false)\n",
      " |-- HadStroke: double (nullable = false)\n",
      " |-- HadAsthma: double (nullable = false)\n",
      " |-- HadSkinCancer: double (nullable = false)\n",
      " |-- HadCOPD: double (nullable = false)\n",
      " |-- HadDepressiveDisorder: double (nullable = false)\n",
      " |-- HadKidneyDisease: double (nullable = false)\n",
      " |-- HadArthritis: double (nullable = false)\n",
      " |-- AlcoholDrinkers: double (nullable = false)\n",
      " |-- HIVTesting: double (nullable = false)\n",
      " |-- FluVaxLast12: double (nullable = false)\n",
      " |-- PneumoVaxEver: double (nullable = false)\n",
      " |-- HadHeartDisease: double (nullable = false)\n",
      " |-- Smoked: double (nullable = false)\n",
      " |-- Diabetic: double (nullable = false)\n",
      " |-- AgeCategory: double (nullable = false)\n",
      " |-- RaceEth: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of features and features created earlier to be dropped\n",
    "features_to_drop = ['HadTetanus', 'HadCovid', 'EcigaFlag', 'HighRiskLastYear', 'features']\n",
    "\n",
    "# Drop the features\n",
    "reduced_df = formatted_df.drop(*features_to_drop)\n",
    "\n",
    "# Verify the remaining columns\n",
    "reduced_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4880015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the 'HadHeartDisease' column to a Pandas DataFrame\n",
    "pandas_df = reduced_df.select('HadHeartDisease').toPandas()\n",
    "\n",
    "# Plot the distribution of 'HadHeartDisease'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=pandas_df, x='HadHeartDisease', palette='viridis')\n",
    "plt.title('Distribution of HadHeartDisease')\n",
    "plt.xlabel('HadHeartDisease')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a084f3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 152:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|HadHeartDisease|count|\n",
      "+---------------+-----+\n",
      "|            0.0|25436|\n",
      "|            1.0|25459|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Separate the DataFrame into majority and minority classes\n",
    "majority_class = reduced_df.filter(F.col('HadHeartDisease') == 0)  \n",
    "minority_class = reduced_df.filter(F.col('HadHeartDisease') == 1)  \n",
    "\n",
    "# Get the size of the minority class\n",
    "minority_count = minority_class.count()\n",
    "\n",
    "# Downsample the majority class to match the size of the minority class\n",
    "majority_class_downsampled = majority_class.sample(False, float(minority_count) / majority_class.count())\n",
    "\n",
    "balanced_df = majority_class_downsampled.union(minority_class)\n",
    "\n",
    "# Verify the balance\n",
    "balanced_df.groupBy('HadHeartDisease').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffd6d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SleepHours: integer (nullable = true)\n",
      " |-- BMI: double (nullable = false)\n",
      " |-- Sex: double (nullable = false)\n",
      " |-- PhysicalActivities: double (nullable = false)\n",
      " |-- HadStroke: double (nullable = false)\n",
      " |-- HadAsthma: double (nullable = false)\n",
      " |-- HadSkinCancer: double (nullable = false)\n",
      " |-- HadCOPD: double (nullable = false)\n",
      " |-- HadDepressiveDisorder: double (nullable = false)\n",
      " |-- HadKidneyDisease: double (nullable = false)\n",
      " |-- HadArthritis: double (nullable = false)\n",
      " |-- AlcoholDrinkers: double (nullable = false)\n",
      " |-- HIVTesting: double (nullable = false)\n",
      " |-- FluVaxLast12: double (nullable = false)\n",
      " |-- PneumoVaxEver: double (nullable = false)\n",
      " |-- HadHeartDisease: double (nullable = false)\n",
      " |-- Smoked: double (nullable = false)\n",
      " |-- Diabetic: double (nullable = false)\n",
      " |-- AgeCategory: double (nullable = false)\n",
      " |-- RaceEth: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "balanced_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe6cacc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas_df = balanced_df.select('HadHeartDisease').toPandas()\n",
    "\n",
    "# Plot the distribution of 'HadHeartDisease'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=pandas_df, x='HadHeartDisease', palette='viridis')\n",
    "plt.title('Distribution of HadHeartDisease')\n",
    "plt.xlabel('HadHeartDisease')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b2c4810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of columns in the balanced DataFrame is: 20\n"
     ]
    }
   ],
   "source": [
    "num_columns = len(balanced_df.columns)\n",
    "print(f\"The number of columns in the balanced DataFrame is: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfa8973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForestClassifier: 0.7291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of RandomForestClassifier: 0.8065\n",
      "Feature Importance:\n",
      "AgeCategory: 0.3783\n",
      "PneumoVaxEver: 0.1608\n",
      "HadArthritis: 0.1058\n",
      "HadStroke: 0.0970\n",
      "Diabetic: 0.0831\n",
      "HadCOPD: 0.0775\n",
      "Sex: 0.0307\n",
      "Smoked: 0.0226\n",
      "HadKidneyDisease: 0.0218\n",
      "PhysicalActivities: 0.0082\n",
      "AlcoholDrinkers: 0.0062\n",
      "HadSkinCancer: 0.0025\n",
      "FluVaxLast12: 0.0023\n",
      "SleepHours: 0.0015\n",
      "BMI: 0.0007\n",
      "HadDepressiveDisorder: 0.0004\n",
      "HadAsthma: 0.0002\n",
      "HIVTesting: 0.0002\n",
      "RaceEth: 0.0001\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Combine all features into a single vector column\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "formatted_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = formatted_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Fit the RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol='HadHeartDisease', featuresCol='features', numTrees=100)\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = rf_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy of RandomForestClassifier: {accuracy:.4f}\")\n",
    "\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of RandomForestClassifier: {auc:.4f}\")\n",
    "\n",
    "# Extract Feature Importances\n",
    "importances = rf_model.featureImportances\n",
    "feature_importances = sorted(zip(importances, feature_columns), reverse=True)\n",
    "\n",
    "# Display Feature Importances in descending order\n",
    "print(\"Feature Importance:\")\n",
    "for importance, feature in feature_importances:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60e5f9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier: 0.7125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of DecisionTreeClassifier: 0.6822\n",
      "Feature Importance:\n",
      "AgeCategory: 0.6844\n",
      "Diabetic: 0.1039\n",
      "HadCOPD: 0.0707\n",
      "HadArthritis: 0.0695\n",
      "HadStroke: 0.0407\n",
      "Sex: 0.0308\n",
      "Smoked: 0.0000\n",
      "SleepHours: 0.0000\n",
      "RaceEth: 0.0000\n",
      "PneumoVaxEver: 0.0000\n",
      "PhysicalActivities: 0.0000\n",
      "HadSkinCancer: 0.0000\n",
      "HadKidneyDisease: 0.0000\n",
      "HadDepressiveDisorder: 0.0000\n",
      "HadAsthma: 0.0000\n",
      "HIVTesting: 0.0000\n",
      "FluVaxLast12: 0.0000\n",
      "BMI: 0.0000\n",
      "AlcoholDrinkers: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Combine all features into a single vector column\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "formatted_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = formatted_df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Fit the DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol='HadHeartDisease', featuresCol='features')\n",
    "dt_model = dt.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = dt_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of DecisionTreeClassifier: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model's AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of DecisionTreeClassifier: {auc:.4f}\")\n",
    "\n",
    "# Extract Feature Importances\n",
    "importances = dt_model.featureImportances\n",
    "feature_importances = sorted(zip(importances, feature_columns), reverse=True)\n",
    "\n",
    "# Display Feature Importances in descending order\n",
    "print(\"Feature Importance:\")\n",
    "for importance, feature in feature_importances:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b29ad0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 02:11:44 WARN DAGScheduler: Broadcasting large task binary with size 1000.2 KiB\n",
      "24/05/20 02:11:44 WARN DAGScheduler: Broadcasting large task binary with size 1001.4 KiB\n",
      "24/05/20 02:11:44 WARN DAGScheduler: Broadcasting large task binary with size 1004.0 KiB\n",
      "24/05/20 02:11:45 WARN DAGScheduler: Broadcasting large task binary with size 1007.6 KiB\n",
      "24/05/20 02:11:45 WARN DAGScheduler: Broadcasting large task binary with size 1008.2 KiB\n",
      "24/05/20 02:11:45 WARN DAGScheduler: Broadcasting large task binary with size 1008.7 KiB\n",
      "24/05/20 02:11:45 WARN DAGScheduler: Broadcasting large task binary with size 1009.9 KiB\n",
      "24/05/20 02:11:45 WARN DAGScheduler: Broadcasting large task binary with size 1012.6 KiB\n",
      "24/05/20 02:11:46 WARN DAGScheduler: Broadcasting large task binary with size 1016.1 KiB\n",
      "24/05/20 02:11:46 WARN DAGScheduler: Broadcasting large task binary with size 1016.6 KiB\n",
      "24/05/20 02:11:46 WARN DAGScheduler: Broadcasting large task binary with size 1017.3 KiB\n",
      "24/05/20 02:11:46 WARN DAGScheduler: Broadcasting large task binary with size 1018.6 KiB\n",
      "24/05/20 02:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1021.0 KiB\n",
      "24/05/20 02:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1024.1 KiB\n",
      "24/05/20 02:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1024.7 KiB\n",
      "24/05/20 02:11:47 WARN DAGScheduler: Broadcasting large task binary with size 1025.2 KiB\n",
      "24/05/20 02:11:48 WARN DAGScheduler: Broadcasting large task binary with size 1026.5 KiB\n",
      "24/05/20 02:11:48 WARN DAGScheduler: Broadcasting large task binary with size 1029.0 KiB\n",
      "24/05/20 02:11:48 WARN DAGScheduler: Broadcasting large task binary with size 1032.5 KiB\n",
      "24/05/20 02:11:48 WARN DAGScheduler: Broadcasting large task binary with size 1033.0 KiB\n",
      "24/05/20 02:11:48 WARN DAGScheduler: Broadcasting large task binary with size 1033.5 KiB\n",
      "24/05/20 02:11:48 WARN DAGScheduler: Broadcasting large task binary with size 1035.0 KiB\n",
      "24/05/20 02:11:49 WARN DAGScheduler: Broadcasting large task binary with size 1037.7 KiB\n",
      "24/05/20 02:11:49 WARN DAGScheduler: Broadcasting large task binary with size 1041.3 KiB\n",
      "24/05/20 02:11:49 WARN DAGScheduler: Broadcasting large task binary with size 1041.8 KiB\n",
      "24/05/20 02:11:50 WARN DAGScheduler: Broadcasting large task binary with size 1042.4 KiB\n",
      "24/05/20 02:11:50 WARN DAGScheduler: Broadcasting large task binary with size 1043.7 KiB\n",
      "24/05/20 02:11:50 WARN DAGScheduler: Broadcasting large task binary with size 1046.2 KiB\n",
      "24/05/20 02:11:50 WARN DAGScheduler: Broadcasting large task binary with size 1049.3 KiB\n",
      "24/05/20 02:11:50 WARN DAGScheduler: Broadcasting large task binary with size 1049.8 KiB\n",
      "24/05/20 02:11:51 WARN DAGScheduler: Broadcasting large task binary with size 1050.4 KiB\n",
      "24/05/20 02:11:51 WARN DAGScheduler: Broadcasting large task binary with size 1051.7 KiB\n",
      "24/05/20 02:11:51 WARN DAGScheduler: Broadcasting large task binary with size 1054.5 KiB\n",
      "24/05/20 02:11:51 WARN DAGScheduler: Broadcasting large task binary with size 1058.1 KiB\n",
      "24/05/20 02:11:52 WARN DAGScheduler: Broadcasting large task binary with size 1058.6 KiB\n",
      "24/05/20 02:11:52 WARN DAGScheduler: Broadcasting large task binary with size 1059.2 KiB\n",
      "24/05/20 02:11:52 WARN DAGScheduler: Broadcasting large task binary with size 1060.6 KiB\n",
      "24/05/20 02:11:52 WARN DAGScheduler: Broadcasting large task binary with size 1063.2 KiB\n",
      "24/05/20 02:11:53 WARN DAGScheduler: Broadcasting large task binary with size 1066.3 KiB\n",
      "24/05/20 02:11:53 WARN DAGScheduler: Broadcasting large task binary with size 1066.9 KiB\n",
      "24/05/20 02:11:53 WARN DAGScheduler: Broadcasting large task binary with size 1067.4 KiB\n",
      "24/05/20 02:11:53 WARN DAGScheduler: Broadcasting large task binary with size 1068.8 KiB\n",
      "24/05/20 02:11:53 WARN DAGScheduler: Broadcasting large task binary with size 1071.3 KiB\n",
      "24/05/20 02:11:54 WARN DAGScheduler: Broadcasting large task binary with size 1074.4 KiB\n",
      "24/05/20 02:11:54 WARN DAGScheduler: Broadcasting large task binary with size 1075.0 KiB\n",
      "24/05/20 02:11:54 WARN DAGScheduler: Broadcasting large task binary with size 1075.5 KiB\n",
      "24/05/20 02:11:55 WARN DAGScheduler: Broadcasting large task binary with size 1076.9 KiB\n",
      "24/05/20 02:11:55 WARN DAGScheduler: Broadcasting large task binary with size 1079.4 KiB\n",
      "24/05/20 02:11:55 WARN DAGScheduler: Broadcasting large task binary with size 1082.5 KiB\n",
      "24/05/20 02:11:55 WARN DAGScheduler: Broadcasting large task binary with size 1083.0 KiB\n",
      "24/05/20 02:11:55 WARN DAGScheduler: Broadcasting large task binary with size 1083.6 KiB\n",
      "24/05/20 02:11:56 WARN DAGScheduler: Broadcasting large task binary with size 1084.9 KiB\n",
      "24/05/20 02:11:56 WARN DAGScheduler: Broadcasting large task binary with size 1087.6 KiB\n",
      "24/05/20 02:11:56 WARN DAGScheduler: Broadcasting large task binary with size 1091.4 KiB\n",
      "24/05/20 02:11:56 WARN DAGScheduler: Broadcasting large task binary with size 1092.0 KiB\n",
      "24/05/20 02:11:56 WARN DAGScheduler: Broadcasting large task binary with size 1092.6 KiB\n",
      "24/05/20 02:11:56 WARN DAGScheduler: Broadcasting large task binary with size 1093.9 KiB\n",
      "24/05/20 02:11:57 WARN DAGScheduler: Broadcasting large task binary with size 1096.4 KiB\n",
      "24/05/20 02:11:58 WARN DAGScheduler: Broadcasting large task binary with size 1087.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gradient-Boosted Tree Classifier: 0.7476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 02:12:12 WARN DAGScheduler: Broadcasting large task binary with size 1079.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Gradient-Boosted Tree Classifier: 0.8191\n",
      "Feature Importance:\n",
      "AgeCategory: 0.2870\n",
      "BMI: 0.0718\n",
      "HadKidneyDisease: 0.0680\n",
      "SleepHours: 0.0635\n",
      "HadArthritis: 0.0622\n",
      "Smoked: 0.0600\n",
      "PneumoVaxEver: 0.0554\n",
      "HadStroke: 0.0464\n",
      "Sex: 0.0449\n",
      "RaceEth: 0.0443\n",
      "Diabetic: 0.0406\n",
      "HadCOPD: 0.0371\n",
      "AlcoholDrinkers: 0.0297\n",
      "HadDepressiveDisorder: 0.0293\n",
      "PhysicalActivities: 0.0243\n",
      "HadAsthma: 0.0115\n",
      "HadSkinCancer: 0.0103\n",
      "FluVaxLast12: 0.0075\n",
      "HIVTesting: 0.0060\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "# Combine all features into a single vector column\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "formatted_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = formatted_df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Fit the Gradient-Boosted Tree Classifier\n",
    "gbt = GBTClassifier(labelCol='HadHeartDisease', featuresCol='features', maxIter=100)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of Gradient-Boosted Tree Classifier: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model's AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of Gradient-Boosted Tree Classifier: {auc:.4f}\")\n",
    "\n",
    "# Extract Feature Importances\n",
    "importances = gbt_model.featureImportances\n",
    "feature_importances = sorted(zip(importances, feature_columns), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Display Feature Importances in descending order\n",
    "print(\"Feature Importance:\")\n",
    "for importance, feature in feature_importances:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a0692e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.7261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Logistic Regression: 0.8008\n",
      "Feature Coefficients:\n",
      "SleepHours: -0.0049\n",
      "BMI: -0.0004\n",
      "Sex: 0.7911\n",
      "PhysicalActivities: 0.3184\n",
      "HadStroke: 1.4189\n",
      "HadAsthma: -0.0167\n",
      "HadSkinCancer: 0.4477\n",
      "HadCOPD: 0.7468\n",
      "HadDepressiveDisorder: 0.1073\n",
      "HadKidneyDisease: 0.8290\n",
      "HadArthritis: 0.5686\n",
      "AlcoholDrinkers: 0.3862\n",
      "HIVTesting: -0.1992\n",
      "FluVaxLast12: -0.0914\n",
      "PneumoVaxEver: 0.6870\n",
      "Smoked: 0.3949\n",
      "Diabetic: 0.6698\n",
      "AgeCategory: -0.0970\n",
      "RaceEth: -0.0367\n",
      "Intercept: -1.4434\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "formatted_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = formatted_df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Fit the Logistic Regression model\n",
    "lr = LogisticRegression(labelCol='HadHeartDisease', featuresCol='features', maxIter=100)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of Logistic Regression: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model's AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of Logistic Regression: {auc:.4f}\")\n",
    "\n",
    "# Extract Feature Coefficients\n",
    "coefficients = lr_model.coefficients.toArray()\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "# Display Feature Coefficients\n",
    "print(\"Feature Coefficients:\")\n",
    "for feature, coefficient in zip(feature_columns, coefficients):\n",
    "    print(f\"{feature}: {coefficient:.4f}\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "790ae83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/20 02:34:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 0.6812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Naive Bayes: 0.4241\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Combine all features into a single vector column\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "formatted_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = formatted_df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Fit the Naive Bayes model\n",
    "nb = NaiveBayes(labelCol='HadHeartDisease', featuresCol='features')\n",
    "nb_model = nb.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = nb_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of Naive Bayes: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model's AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of Naive Bayes: {auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60d17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
