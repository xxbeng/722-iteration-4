{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c5c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/22 23:49:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/22 23:49:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('heart-disease-mining-clean') \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6cc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dirt= spark.read.csv('Datasets/heart_2022_with_nans - with dirt.csv', header=True, inferSchema=True)\n",
    "#for column in df_dirt.columns:\n",
    "    #null_count = df_dirt.filter(df_dirt[column].isNull()).count()\n",
    "    #print(f\"Column '{column}': {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1032f3e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        SmokerStatus| count|\n",
      "+--------------------+------+\n",
      "|        Never smoked|245955|\n",
      "|       Former smoker|113769|\n",
      "|Current smoker - ...| 36003|\n",
      "|                null| 35462|\n",
      "|Current smoker - ...| 13938|\n",
      "|     Previous smoker|     5|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# use groupby to calculate value counts for 'SmokerStatus'\n",
    "smoker_status_counts = df_dirt.groupBy('SmokerStatus').count().orderBy('count', ascending=False)\n",
    "smoker_status_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f44aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|    AgeCategory|count|\n",
      "+---------------+-----+\n",
      "|   Age 65 to 69|47098|\n",
      "|   Age 60 to 64|44511|\n",
      "|   Age 70 to 74|43472|\n",
      "|   Age 55 to 59|36821|\n",
      "|Age 80 or older|36250|\n",
      "|   Age 50 to 54|33644|\n",
      "|   Age 75 to 79|32516|\n",
      "|   Age 40 to 44|29942|\n",
      "|   Age 45 to 49|28531|\n",
      "|   Age 35 to 39|28526|\n",
      "|   Age 18 to 24|26941|\n",
      "|   Age 30 to 34|25806|\n",
      "|   Age 25 to 29|21989|\n",
      "|           null| 9079|\n",
      "|             78|    1|\n",
      "|             67|    1|\n",
      "|             76|    1|\n",
      "|             88|    1|\n",
      "|             34|    1|\n",
      "|             26|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:=============================>                             (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Age_counts = df_dirt.groupBy('AgeCategory').count().orderBy('count', ascending=False)\n",
    "Age_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc37311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|WeightInKilograms|\n",
      "+-------+-----------------+\n",
      "|  count|           403054|\n",
      "|   mean|83.10065152559405|\n",
      "| stddev|27.15749401703654|\n",
      "|    min|            22.68|\n",
      "|    max|          10659.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dirt.describe('WeightInKilograms').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246c85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "columns_to_exclude = [\n",
    "    'State', 'LastCheckupTime', 'ChestScan', 'GeneralHealth',\n",
    "    'PhysicalHealthDays', 'MentalHealthDays', 'RemovedTeeth',\n",
    "    'DeafOrHardOfHearing', 'BlindOrVisionDifficulty',\n",
    "    'DifficultyConcentrating', 'DifficultyWalking',\n",
    "    'DifficultyDressingBathing', 'DifficultyErrands',\n",
    "    'HeightInMeters', 'WeightInKilograms'  \n",
    "]\n",
    "df_03 = df_dirt.drop(*columns_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae9bfa08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sex',\n",
       " 'PhysicalActivities',\n",
       " 'SleepHours',\n",
       " 'HadHeartAttack',\n",
       " 'HadAngina',\n",
       " 'HadStroke',\n",
       " 'HadAsthma',\n",
       " 'HadSkinCancer',\n",
       " 'HadCOPD',\n",
       " 'HadDepressiveDisorder',\n",
       " 'HadKidneyDisease',\n",
       " 'HadArthritis',\n",
       " 'HadDiabetes',\n",
       " 'SmokerStatus',\n",
       " 'ECigaretteUsage',\n",
       " 'RaceEthnicityCategory',\n",
       " 'AgeCategory',\n",
       " 'BMI',\n",
       " 'AlcoholDrinkers',\n",
       " 'HIVTesting',\n",
       " 'FluVaxLast12',\n",
       " 'PneumoVaxEver',\n",
       " 'TetanusLast10Tdap',\n",
       " 'HighRiskLastYear',\n",
       " 'CovidPos']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_03.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d325bd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 1093\n",
      "SleepHours: 5453\n",
      "HadHeartAttack: 3065\n",
      "HadAngina: 4405\n",
      "HadStroke: 1557\n",
      "HadAsthma: 1773\n",
      "HadSkinCancer: 3143\n",
      "HadCOPD: 2219\n",
      "HadDepressiveDisorder: 2812\n",
      "HadKidneyDisease: 1926\n",
      "HadArthritis: 2633\n",
      "HadDiabetes: 1087\n",
      "SmokerStatus: 35462\n",
      "ECigaretteUsage: 35660\n",
      "RaceEthnicityCategory: 14057\n",
      "AgeCategory: 9079\n",
      "BMI: 48806\n",
      "AlcoholDrinkers: 46574\n",
      "HIVTesting: 66127\n",
      "FluVaxLast12: 47121\n",
      "PneumoVaxEver: 77040\n",
      "TetanusLast10Tdap: 82516\n",
      "HighRiskLastYear: 50623\n",
      "CovidPos: 50764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create a list of columns with their respective null counts using isnull\n",
    "null_counts = df_03.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8573a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0.00%\n",
      "PhysicalActivities: 0.25%\n",
      "SleepHours: 1.23%\n",
      "HadHeartAttack: 0.69%\n",
      "HadAngina: 0.99%\n",
      "HadStroke: 0.35%\n",
      "HadAsthma: 0.40%\n",
      "HadSkinCancer: 0.71%\n",
      "HadCOPD: 0.50%\n",
      "HadDepressiveDisorder: 0.63%\n",
      "HadKidneyDisease: 0.43%\n",
      "HadArthritis: 0.59%\n",
      "HadDiabetes: 0.24%\n",
      "SmokerStatus: 7.97%\n",
      "ECigaretteUsage: 8.01%\n",
      "RaceEthnicityCategory: 3.16%\n",
      "AgeCategory: 2.04%\n",
      "BMI: 10.96%\n",
      "AlcoholDrinkers: 10.46%\n",
      "HIVTesting: 14.86%\n",
      "FluVaxLast12: 10.59%\n",
      "PneumoVaxEver: 17.31%\n",
      "TetanusLast10Tdap: 18.54%\n",
      "HighRiskLastYear: 11.37%\n",
      "CovidPos: 11.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the total number of rows in the DataFrame\n",
    "total_count = df_03.count()\n",
    "\n",
    "# Calculate the number of null values and the percentage of nulls for each column\n",
    "null_percentage = df_03.select([\n",
    "    ((F.count(F.when(F.col(c).isNull(), c)) / total_count) * 100).alias(c)\n",
    "    for c in df_03.columns\n",
    "])\n",
    "\n",
    "null_percentage_dict = null_percentage.collect()[0].asDict()\n",
    "\n",
    "for column, percentage in null_percentage_dict.items():\n",
    "    print(f\"{column}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9604e34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 0\n",
      "SleepHours: 2221\n",
      "HadHeartAttack: 0\n",
      "HadAngina: 0\n",
      "HadStroke: 0\n",
      "HadAsthma: 0\n",
      "HadSkinCancer: 0\n",
      "HadCOPD: 0\n",
      "HadDepressiveDisorder: 0\n",
      "HadKidneyDisease: 0\n",
      "HadArthritis: 0\n",
      "HadDiabetes: 0\n",
      "SmokerStatus: 0\n",
      "ECigaretteUsage: 0\n",
      "RaceEthnicityCategory: 0\n",
      "AgeCategory: 0\n",
      "BMI: 15757\n",
      "AlcoholDrinkers: 0\n",
      "HIVTesting: 0\n",
      "FluVaxLast12: 0\n",
      "PneumoVaxEver: 0\n",
      "TetanusLast10Tdap: 0\n",
      "HighRiskLastYear: 0\n",
      "CovidPos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 20:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# List of categorical columns to check for null values\n",
    "categorical_columns = [\n",
    "    'Sex', 'PhysicalActivities', 'HadHeartAttack', 'HadAngina', 'HadStroke', 'HadAsthma', \n",
    "    'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder', 'HadKidneyDisease', 'HadArthritis', \n",
    "    'HadDiabetes', 'SmokerStatus', 'ECigaretteUsage', 'RaceEthnicityCategory', 'AgeCategory', \n",
    "    'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver', 'TetanusLast10Tdap', \n",
    "    'HighRiskLastYear', 'CovidPos'\n",
    "]\n",
    "\n",
    "df_03_cleaned_01= df_03.na.drop(subset=categorical_columns)\n",
    "\n",
    "# Count null values in each column after dropping rows\n",
    "null_counts = df_03_cleaned_01.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03_cleaned_01.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "# Print the null counts for each column\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93dcb016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex: 0\n",
      "PhysicalActivities: 0\n",
      "SleepHours: 0\n",
      "HadHeartAttack: 0\n",
      "HadAngina: 0\n",
      "HadStroke: 0\n",
      "HadAsthma: 0\n",
      "HadSkinCancer: 0\n",
      "HadCOPD: 0\n",
      "HadDepressiveDisorder: 0\n",
      "HadKidneyDisease: 0\n",
      "HadArthritis: 0\n",
      "HadDiabetes: 0\n",
      "SmokerStatus: 0\n",
      "ECigaretteUsage: 0\n",
      "RaceEthnicityCategory: 0\n",
      "AgeCategory: 0\n",
      "BMI: 0\n",
      "AlcoholDrinkers: 0\n",
      "HIVTesting: 0\n",
      "FluVaxLast12: 0\n",
      "PneumoVaxEver: 0\n",
      "TetanusLast10Tdap: 0\n",
      "HighRiskLastYear: 0\n",
      "CovidPos: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Calculate the mean of BMI and SleepHours\n",
    "bmi_mean = df_03_cleaned_01.select(mean('BMI')).first()[0]\n",
    "sleep_hours_mean = df_03_cleaned_01.select(mean('SleepHours')).first()[0]\n",
    "\n",
    "# Replace null values with the mean\n",
    "df_03_cleaned_02 = df_03_cleaned_01.fillna({'BMI': bmi_mean, 'SleepHours': sleep_hours_mean})\n",
    "\n",
    "# Verify the changes by counting null values again\n",
    "null_counts = df_03_cleaned_02.select([F.count(F.when(F.col(c).isNull(), 1)).alias(c) for c in df_03_cleaned_02.columns])\n",
    "null_counts_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "# Print the null counts for each column\n",
    "for column, null_count in null_counts_dict.items():\n",
    "    print(f\"{column}: {null_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5595d13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns before filtering: 288444 rows x 25 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns after filtering: 285788 rows x 25 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bmi_lower_bound = df_03_cleaned_02.approxQuantile(\"BMI\", [0.0025], 0.0)[0]\n",
    "bmi_upper_bound = df_03_cleaned_02.approxQuantile(\"BMI\", [0.9975], 0.0)[0]\n",
    "\n",
    "sleep_lower_bound = df_03_cleaned_02.approxQuantile(\"SleepHours\", [0.0025], 0.0)[0]\n",
    "sleep_upper_bound = df_03_cleaned_02.approxQuantile(\"SleepHours\", [0.9975], 0.0)[0]\n",
    "\n",
    "# Filter the dataframe based on the calculated bounds\n",
    "df_03_cleaned_03 = df_03_cleaned_02.filter(\n",
    "    (F.col('BMI') >= bmi_lower_bound) & (F.col('BMI') <= bmi_upper_bound) &\n",
    "    (F.col('SleepHours') >= sleep_lower_bound) & (F.col('SleepHours') <= sleep_upper_bound)\n",
    ")\n",
    "\n",
    "# Show the number of rows and columns after filtering\n",
    "print(f\"Number of rows and columns before filtering: {df_03_cleaned_02.count()} rows x {len(df_03_cleaned_02.columns)} columns\")\n",
    "print(f\"Number of rows and columns after filtering: {df_03_cleaned_03.count()} rows x {len(df_03_cleaned_03.columns)} columns\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3131ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               BMI|\n",
      "+-------+------------------+\n",
      "|  count|            285788|\n",
      "|   mean|28.596861735345282|\n",
      "| stddev| 6.059249795657394|\n",
      "|    min|             16.24|\n",
      "|    max|             57.39|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_03_cleaned_03.describe('BMI').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed7e4a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|    AgeCategory|\n",
      "+---------------+\n",
      "|   Age 45 to 49|\n",
      "|   Age 25 to 29|\n",
      "|   Age 70 to 74|\n",
      "|   Age 55 to 59|\n",
      "|   Age 18 to 24|\n",
      "|   Age 60 to 64|\n",
      "|   Age 50 to 54|\n",
      "|   Age 35 to 39|\n",
      "|   Age 30 to 34|\n",
      "|Age 80 or older|\n",
      "|   Age 40 to 44|\n",
      "|   Age 75 to 79|\n",
      "|   Age 65 to 69|\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to reclassify numerical ages to age categories\n",
    "def reclassify_age(age):\n",
    "    if age <= 25:\n",
    "        return 'Age 18 to 24'\n",
    "    elif 25 < age <= 30:\n",
    "        return 'Age 25 to 29'\n",
    "    elif 30 < age <= 35:\n",
    "        return 'Age 30 to 34'\n",
    "    elif 35 < age <= 40:\n",
    "        return 'Age 35 to 39'\n",
    "    elif 40 < age <= 45:\n",
    "        return 'Age 40 to 44'\n",
    "    elif 45 < age <= 50:\n",
    "        return 'Age 45 to 49'\n",
    "    elif 50 < age <= 55:\n",
    "        return 'Age 50 to 54'\n",
    "    elif 55 < age <= 60:\n",
    "        return 'Age 55 to 59'\n",
    "    elif 60 < age <= 65:\n",
    "        return 'Age 60 to 64'\n",
    "    elif 65 < age <= 70:\n",
    "        return 'Age 65 to 69'\n",
    "    elif 70 < age <= 75:\n",
    "        return 'Age 70 to 74'\n",
    "    elif 75 < age <= 80:\n",
    "        return 'Age 75 to 79'\n",
    "    else:\n",
    "        return 'Age 80 or older'\n",
    "\n",
    "# UDF to apply the reclassification function\n",
    "reclassify_age_udf = F.udf(lambda x: reclassify_age(int(x)) if x.isdigit() else x, StringType())\n",
    "df_03_cleaned_04 = df_03_cleaned_03.withColumn('AgeCategory', reclassify_age_udf(F.col('AgeCategory')))\n",
    "\n",
    "# Show the reclassified age category\n",
    "df_03_cleaned_04.select('AgeCategory').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb0c7c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        SmokerStatus|\n",
      "+--------------------+\n",
      "|        Never smoked|\n",
      "|Current smoker - ...|\n",
      "|       Former smoker|\n",
      "|Current smoker - ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Function to reclassify smoker status\n",
    "def reclassify_smoker(status):\n",
    "    if status == 'Previous smoker':\n",
    "        return 'Former smoker'\n",
    "    else:\n",
    "        return status\n",
    "\n",
    "# UDF to apply the reclassification function\n",
    "reclassify_smoker_udf = F.udf(lambda x: reclassify_smoker(x), StringType())\n",
    "\n",
    "# Apply the UDF to reclassify smoker status\n",
    "df_03_cleaned_05 = df_03_cleaned_04.withColumn('SmokerStatus', reclassify_smoker_udf(F.col('SmokerStatus')))\n",
    "\n",
    "# Show the reclassified DataFrame\n",
    "df_03_cleaned_05.select('SmokerStatus').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7cb61ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------------+\n",
      "|HadHeartAttack|HadAngina|HadHeartDisease|\n",
      "+--------------+---------+---------------+\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|           Yes|       No|            Yes|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|            No|       No|             No|\n",
      "|           Yes|      Yes|            Yes|\n",
      "|            No|       No|             No|\n",
      "|            No|      Yes|            Yes|\n",
      "|            No|       No|             No|\n",
      "|           Yes|      Yes|            Yes|\n",
      "+--------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Function to determine if a respondent had heart disease\n",
    "def had_heart_disease(had_heart_attack, had_angina):\n",
    "    return 'Yes' if had_heart_attack == 'Yes' or had_angina == 'Yes' else 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_heart_disease_udf = F.udf(lambda x, y: had_heart_disease(x, y), StringType())\n",
    "\n",
    "# Apply the UDF to create the HadHeartDisease column\n",
    "df_03_cleaned_06 = df_03_cleaned_05.withColumn('HadHeartDisease', had_heart_disease_udf(F.col('HadHeartAttack'), F.col('HadAngina')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_06.select('HadHeartAttack', 'HadAngina', 'HadHeartDisease').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ab572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|     ECigaretteUsage|EcigaFlag|\n",
      "+--------------------+---------+\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Not at all (right...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "|Never used e-ciga...|       No|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition two: EcigaFlag column\n",
    "\n",
    "# Function to determine the e-cigarette usage flag\n",
    "def ecig_new(response):\n",
    "    if response in [\"Use them every day\", \"Use them some days\"]:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "ecig_new_udf = F.udf(lambda x: ecig_new(x), StringType())\n",
    "\n",
    "# Apply the UDF to create the EcigaFlag column\n",
    "df_03_cleaned_07 = df_03_cleaned_06.withColumn('EcigaFlag', ecig_new_udf(F.col('ECigaretteUsage')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_07.select('ECigaretteUsage', 'EcigaFlag').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be6dc244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|        SmokerStatus|Smoked|\n",
      "+--------------------+------+\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|        Never smoked|    No|\n",
      "|Current smoker - ...|   Yes|\n",
      "|        Never smoked|    No|\n",
      "|Current smoker - ...|   Yes|\n",
      "|       Former smoker|   Yes|\n",
      "|        Never smoked|    No|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition three: Smoked column\n",
    "# Function to determine if the person has smoked\n",
    "def smoker_new(status):\n",
    "    if status == \"Never smoked\":\n",
    "        return \"No\"\n",
    "    else:\n",
    "        return \"Yes\"\n",
    "\n",
    "# Register the function as a UDF\n",
    "smoker_new_udf = F.udf(lambda x: smoker_new(x), StringType())\n",
    "\n",
    "# Apply the UDF to create the Smoked column\n",
    "df_03_cleaned_08 = df_03_cleaned_07.withColumn('Smoked', smoker_new_udf(F.col('SmokerStatus')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_08.select('SmokerStatus', 'Smoked').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3530475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|HadDiabetes|Diabetic|\n",
      "+-----------+--------+\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "|         No|      No|\n",
      "|        Yes|     Yes|\n",
      "|         No|      No|\n",
      "+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition four: Diabetic column\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "# Function to determine if the person is diabetic\n",
    "def diabetes_new(status):\n",
    "    if status == 'Yes' or status == 'Yes, but only during pregnancy (female)':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "diabetes_new_udf = udf(diabetes_new, StringType())\n",
    "\n",
    "# Apply the UDF to create the Diabetic column\n",
    "df_03_cleaned_09 = df_03_cleaned_08.withColumn('Diabetic', diabetes_new_udf(F.col('HadDiabetes')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_09.select('HadDiabetes', 'Diabetic').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb94c244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Diabetic|\n",
      "+--------+\n",
      "|      No|\n",
      "|     Yes|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_03_cleaned_09.select('Diabetic').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3e443a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|CovidPos|HadCovid|\n",
      "+--------+--------+\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|     Yes|     Yes|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|     Yes|     Yes|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "|      No|      No|\n",
      "+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition five: had_covid\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to determine if the person had Covid\n",
    "def had_covid(status):\n",
    "    if status == 'Yes' or status == 'Tested positive using home test without a health professional':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_covid_udf = udf(had_covid, StringType())\n",
    "\n",
    "# Apply the UDF to create the HadCovid column\n",
    "df_03_cleaned_10 = df_03_cleaned_09.withColumn('HadCovid', had_covid_udf(F.col('CovidPos')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_10.select('CovidPos', 'HadCovid').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7123c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|   TetanusLast10Tdap|HadTetanus|\n",
      "+--------------------+----------+\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|  Yes, received Tdap|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|Yes, received tet...|       Yes|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|No, did not recei...|        No|\n",
      "|Yes, received tet...|       Yes|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition six\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to determine if the person had tetanus\n",
    "def had_tetanus(status):\n",
    "    if status == 'Yes, received tetanus shot but not sure what type' or status == 'Yes, received Tdap':\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "# Register the function as a UDF\n",
    "had_tetanus_udf = udf(had_tetanus, StringType())\n",
    "\n",
    "# Apply the UDF to create the HadTetanus column\n",
    "df_03_cleaned_11 = df_03_cleaned_10.withColumn('HadTetanus', had_tetanus_udf(F.col('TetanusLast10Tdap')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_11.select('TetanusLast10Tdap', 'HadTetanus').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b19a66c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|RaceEthnicityCategory|RaceEth|\n",
      "+---------------------+-------+\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| Black only, Non-H...|  Black|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "| White only, Non-H...|  White|\n",
      "+---------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 60:>                                                         (0 + 1) / 1]\r",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/ubuntu/spark-3.2.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Addition seven\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Function to reclassify race and ethnicity\n",
    "def race_ethnicity(category):\n",
    "    if category == 'White only, Non-Hispanic':\n",
    "        return 'White'\n",
    "    elif category == 'Black only, Non-Hispanic':\n",
    "        return 'Black'\n",
    "    elif category == 'Other race only, Non-Hispanic':\n",
    "        return 'Other'\n",
    "    elif category == 'Hispanic':\n",
    "        return 'Hispanic'\n",
    "    elif category == 'Multiracial, Non-Hispanic':\n",
    "        return 'Multiracial'\n",
    "    else:\n",
    "        return category\n",
    "\n",
    "# Register the function as a UDF\n",
    "race_ethnicity_udf = udf(race_ethnicity, StringType())\n",
    "\n",
    "# Apply the UDF to create the RaceEth column\n",
    "df_03_cleaned_12 = df_03_cleaned_11.withColumn('RaceEth', race_ethnicity_udf(F.col('RaceEthnicityCategory')))\n",
    "\n",
    "# Show the result\n",
    "df_03_cleaned_12.select('RaceEthnicityCategory', 'RaceEth').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c1bd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- PhysicalActivities: string (nullable = true)\n",
      " |-- SleepHours: integer (nullable = true)\n",
      " |-- HadStroke: string (nullable = true)\n",
      " |-- HadAsthma: string (nullable = true)\n",
      " |-- HadSkinCancer: string (nullable = true)\n",
      " |-- HadCOPD: string (nullable = true)\n",
      " |-- HadDepressiveDisorder: string (nullable = true)\n",
      " |-- HadKidneyDisease: string (nullable = true)\n",
      " |-- HadArthritis: string (nullable = true)\n",
      " |-- AgeCategory: string (nullable = true)\n",
      " |-- BMI: double (nullable = false)\n",
      " |-- AlcoholDrinkers: string (nullable = true)\n",
      " |-- HIVTesting: string (nullable = true)\n",
      " |-- FluVaxLast12: string (nullable = true)\n",
      " |-- PneumoVaxEver: string (nullable = true)\n",
      " |-- HighRiskLastYear: string (nullable = true)\n",
      " |-- HadHeartDisease: string (nullable = true)\n",
      " |-- EcigaFlag: string (nullable = true)\n",
      " |-- Smoked: string (nullable = true)\n",
      " |-- Diabetic: string (nullable = true)\n",
      " |-- HadCovid: string (nullable = true)\n",
      " |-- HadTetanus: string (nullable = true)\n",
      " |-- RaceEth: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.5 Data formatting\n",
    "cleaned_df = df_03_cleaned_12\n",
    "# Columns to drop\n",
    "columns_to_drop = [\"ECigaretteUsage\", \"SmokerStatus\", \n",
    "                   \"HadHeartAttack\", \"HadAngina\", \"HadDiabetes\", \"CovidPos\", \"TetanusLast10Tdap\", \"RaceEthnicityCategory\"]\n",
    "\n",
    "# Drop the columns\n",
    "formatted_df_01 = cleaned_df.drop(*columns_to_drop)\n",
    "\n",
    "# Show the schema to verify columns are dropped\n",
    "formatted_df_01.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Sex to Index:\n",
      "  0 -> Female\n",
      "  1 -> Male\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of PhysicalActivities to Index:\n",
      "  0 -> Yes\n",
      "  1 -> No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadStroke to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadAsthma to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadSkinCancer to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadCOPD to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of HadDepressiveDisorder to Index:\n",
      "  0 -> No\n",
      "  1 -> Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 82:>                                                         (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Define all categorical columns, excluding AgeCategory for manual indexing\n",
    "categorical_columns = [\n",
    "    'Sex', 'PhysicalActivities', 'HadStroke', 'HadAsthma', 'HadSkinCancer', 'HadCOPD', 'HadDepressiveDisorder',\n",
    "    'HadKidneyDisease', 'HadArthritis', 'AlcoholDrinkers', 'HIVTesting', 'FluVaxLast12', 'PneumoVaxEver',\n",
    "    'HighRiskLastYear', 'HadHeartDisease', 'EcigaFlag', 'Smoked', 'Diabetic', 'HadCovid', 'HadTetanus', 'RaceEth'\n",
    "]\n",
    "\n",
    "# Apply StringIndexer for all categorical columns except AgeCategory\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_Index\") for column in categorical_columns]\n",
    "\n",
    "# Fit the StringIndexer models and transform the data\n",
    "for indexer in indexers:\n",
    "    model = indexer.fit(formatted_df_01)\n",
    "    formatted_df_01 = model.transform(formatted_df_01)\n",
    "    \n",
    "    # Print the labels and their corresponding indices\n",
    "    labels = model.labels\n",
    "    print(f\"Mapping of {indexer.getInputCol()} to Index:\")\n",
    "    for index, label in enumerate(labels):\n",
    "        print(f\"  {index} -> {label}\")\n",
    "\n",
    "# Manual encoding for AgeCategory\n",
    "age_category_mapping = {\n",
    "    'Age 18 to 24': 0,\n",
    "    'Age 25 to 29': 1,\n",
    "    'Age 30 to 34': 2,\n",
    "    'Age 35 to 39': 3,\n",
    "    'Age 40 to 44': 4,\n",
    "    'Age 45 to 49': 5,\n",
    "    'Age 50 to 54': 6,\n",
    "    'Age 55 to 59': 7,\n",
    "    'Age 60 to 64': 8,\n",
    "    'Age 65 to 69': 9,\n",
    "    'Age 70 to 74': 10,\n",
    "    'Age 75 to 79': 11,\n",
    "    'Age 80 or older': 12\n",
    "}\n",
    "\n",
    "# Create a new column AgeCategory_Index based on the manual mapping\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def age_category_index(age_category):\n",
    "    return age_category_mapping[age_category]\n",
    "\n",
    "# Register the UDF\n",
    "age_category_index_udf = udf(age_category_index, IntegerType())\n",
    "\n",
    "# Apply the UDF to create the new column\n",
    "formatted_df_01 = formatted_df_01.withColumn(\"AgeCategory_Index\", age_category_index_udf(formatted_df_01[\"AgeCategory\"]))\n",
    "\n",
    "# Print the manual mapping\n",
    "print(\"Mapping of AgeCategory to Index:\")\n",
    "for label, index in age_category_mapping.items():\n",
    "    print(f\"  {index} -> {label}\")\n",
    "\n",
    "# Drop the original categorical columns and rename the indexed columns\n",
    "for column in categorical_columns:\n",
    "    formatted_df_01 = formatted_df_01.drop(column).withColumnRenamed(column + \"_Index\", column)\n",
    "\n",
    "# Drop the original AgeCategory column\n",
    "formatted_df_01 = formatted_df_01.drop('AgeCategory').withColumnRenamed('AgeCategory_Index', 'AgeCategory')\n",
    "\n",
    "# Show the schema to confirm changes\n",
    "formatted_df_01.printSchema()\n",
    "\n",
    "# Verify the first few rows to confirm changes\n",
    "formatted_df_01.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad441c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df_01.groupBy('HadHeartDisease').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277e1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iteration 2\n",
    "formatted_df = formatted_df_01\n",
    "features_to_drop = ['HadTetanus', 'HadCovid', 'EcigaFlag', 'HighRiskLastYear', 'features']\n",
    "\n",
    "# to not drop the features\n",
    "reduced_df = formatted_df\n",
    "\n",
    "# Verify the remaining columns\n",
    "reduced_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4880015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert the 'HadHeartDisease' column to a Pandas DataFrame\n",
    "pandas_df = reduced_df.select('HadHeartDisease').toPandas()\n",
    "\n",
    "# Plot the distribution of 'HadHeartDisease'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=pandas_df, x='HadHeartDisease', palette='viridis')\n",
    "plt.title('Distribution of HadHeartDisease')\n",
    "plt.xlabel('HadHeartDisease')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a084f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Separate the DataFrame into majority and minority classes\n",
    "majority_class = reduced_df.filter(F.col('HadHeartDisease') == 0)  \n",
    "minority_class = reduced_df.filter(F.col('HadHeartDisease') == 1)  \n",
    "\n",
    "# Get the size of the minority class\n",
    "minority_count = minority_class.count()\n",
    "\n",
    "# Downsample the majority class to match the size of the minority class\n",
    "majority_class_downsampled = majority_class.sample(False, float(minority_count) / majority_class.count())\n",
    "\n",
    "balanced_df = majority_class_downsampled.union(minority_class)\n",
    "\n",
    "# Verify the balance\n",
    "balanced_df.groupBy('HadHeartDisease').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "# Combine all features into a single vector column\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "model_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = model_df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Fit the Gradient-Boosted Tree Classifier\n",
    "gbt = GBTClassifier(labelCol='HadHeartDisease', featuresCol='features', maxIter=100)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of Gradient-Boosted Tree Classifier: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model's AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of Gradient-Boosted Tree Classifier: {auc:.4f}\")\n",
    "\n",
    "# Extract Feature Importances\n",
    "importances = gbt_model.featureImportances\n",
    "feature_importances = sorted(zip(importances, feature_columns), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Display Feature Importances in descending order\n",
    "print(\"Feature Importance:\")\n",
    "for importance, feature in feature_importances:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0692e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "feature_columns = [col for col in balanced_df.columns if col != 'HadHeartDisease']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "model_df = assembler.transform(balanced_df)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train_df, test_df) = model_df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Fit the Logistic Regression model\n",
    "lr = LogisticRegression(labelCol='HadHeartDisease', featuresCol='features', maxIter=100)\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(labelCol='HadHeartDisease', predictionCol='prediction', metricName='accuracy')\n",
    "accuracy = accuracy_evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of Logistic Regression: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model's AUC\n",
    "auc_evaluator = BinaryClassificationEvaluator(labelCol='HadHeartDisease', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "auc = auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC of Logistic Regression: {auc:.4f}\")\n",
    "\n",
    "# Extract Feature Coefficients\n",
    "coefficients = lr_model.coefficients.toArray()\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "# Display Feature Coefficients\n",
    "print(\"Feature Coefficients:\")\n",
    "for feature, coefficient in zip(feature_columns, coefficients):\n",
    "    print(f\"{feature}: {coefficient:.4f}\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6791e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
